{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNokDagEFLfAYmiwLK7/7FQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/redfear08/bot_trade_ml/blob/main/trading_bot_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install -q pandas scikit-learn tensorflow kiteconnect keras-tuner"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3Wp1sPzWFge",
        "outputId": "91ede822-69e7-4904-e242-26244b7590f4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m771.5/771.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.8/247.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Authentication and data fetching\n",
        "import pandas as pd\n",
        "from kiteconnect import KiteConnect\n",
        "import datetime as dt\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "api_key = 'klz728yv89qrljzs'\n",
        "api_secret = '4vhxunujbp17i8da0y1tiy7ayde4h5o8'\n",
        "kite = KiteConnect(api_key=api_key)\n",
        "print(\"Login URL:\", kite.login_url())\n",
        "request_token = input(\"Enter request token: \")\n",
        "data = kite.generate_session(request_token, api_secret=api_secret)\n",
        "access_token = data[\"access_token\"]\n",
        "kite.set_access_token(access_token)\n",
        "\n",
        "# Fetch historical data\n",
        "def fetch_historical_data(kite, instrument_token, start_date, end_date, interval, csv_filename):\n",
        "    delta = dt.timedelta(days=60)\n",
        "    current_date = start_date\n",
        "    all_data = []\n",
        "\n",
        "    while current_date < end_date:\n",
        "        to_date = min(current_date + delta, end_date)\n",
        "        data = kite.historical_data(instrument_token, current_date, to_date, interval)\n",
        "        if data:\n",
        "            all_data.extend(data)\n",
        "        else:\n",
        "            print(f\"No data fetched for period {current_date} to {to_date}\")\n",
        "        current_date = to_date + dt.timedelta(days=1)\n",
        "        time.sleep(1)  # Avoid hitting API rate limits\n",
        "\n",
        "    if not all_data:\n",
        "        print(\"No data fetched at all\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    df = pd.DataFrame(all_data)\n",
        "    df.to_csv(csv_filename, mode='w', index=False, header=True)\n",
        "    return df\n",
        "\n",
        "# Fetch historical data\n",
        "instrument_token = '256265'  # INFY\n",
        "start_date = dt.datetime(2016, 1, 1)\n",
        "end_date = dt.datetime(2023, 12, 31)\n",
        "interval = 'minute'\n",
        "csv_filename = 'historical_data.csv'\n",
        "\n",
        "df = fetch_historical_data(kite, instrument_token, start_date, end_date, interval, csv_filename)\n",
        "print(\"Data fetched:\")\n",
        "print(df.head())\n",
        "\n",
        "if df.empty:\n",
        "    print(\"The fetched data is empty. Exiting.\")\n",
        "    exit()\n",
        "\n",
        "# Feature Engineering\n",
        "def add_features(df):\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    df.set_index('date', inplace=True)\n",
        "\n",
        "    df['SMA_50'] = df['close'].rolling(window=50).mean()\n",
        "    print(\"SMA_50 added:\")\n",
        "    print(df[['SMA_50']].head(60))\n",
        "\n",
        "    df['SMA_200'] = df['close'].rolling(window=200).mean()\n",
        "    print(\"SMA_200 added:\")\n",
        "    print(df[['SMA_200']].head(210))\n",
        "\n",
        "    df['RSI'] = calculate_rsi(df, 14)\n",
        "    print(\"RSI added:\")\n",
        "    print(df[['RSI']].head(20))\n",
        "\n",
        "    df['Bollinger_Upper'], df['Bollinger_Lower'] = calculate_bollinger_bands(df, 20)\n",
        "    print(\"Bollinger Bands added:\")\n",
        "    print(df[['Bollinger_Upper', 'Bollinger_Lower']].head(25))\n",
        "\n",
        "    df['MACD'], df['MACD_Signal'] = calculate_macd(df)\n",
        "    print(\"MACD added:\")\n",
        "    print(df[['MACD', 'MACD_Signal']].head(30))\n",
        "\n",
        "    df['Stochastic_%K'], df['Stochastic_%D'] = calculate_stochastic_oscillator(df, 14)\n",
        "    print(\"Stochastic Oscillator added:\")\n",
        "    print(df[['Stochastic_%K', 'Stochastic_%D']].head(20))\n",
        "\n",
        "    df['Momentum'] = df['close'] / df['close'].shift(10) - 1\n",
        "    print(\"Momentum added:\")\n",
        "    print(df[['Momentum']].head(15))\n",
        "\n",
        "    df['VWAP'] = calculate_vwap(df)\n",
        "    print(\"VWAP added:\")\n",
        "    print(df[['VWAP']].head(15))\n",
        "\n",
        "    df['ATR'] = calculate_atr(df)\n",
        "    print(\"ATR added:\")\n",
        "    print(df[['ATR']].head(15))\n",
        "\n",
        "    df['Target'] = np.where(df['close'].shift(-1) > df['close'], 1, 0)\n",
        "    print(\"Target added:\")\n",
        "    print(df[['Target']].head(15))\n",
        "\n",
        "    print(\"DataFrame before dropping NaNs:\")\n",
        "    print(df.head(220))\n",
        "\n",
        "    df.dropna(subset=['Target'], inplace=True)\n",
        "    print(\"DataFrame after dropping NaNs:\")\n",
        "    print(df.head(220))\n",
        "\n",
        "    return df\n",
        "\n",
        "def calculate_rsi(df, window=14):\n",
        "    delta = df['close'].diff(1)\n",
        "    gain = (delta.where(delta > 0, 0)).fillna(0)\n",
        "    loss = (-delta.where(delta < 0, 0)).fillna(0)\n",
        "    avg_gain = gain.rolling(window=window).mean()\n",
        "    avg_loss = loss.rolling(window=window).mean()\n",
        "    rs = avg_gain / avg_loss\n",
        "    return 100 - (100 / (1 + rs))\n",
        "\n",
        "def calculate_bollinger_bands(df, window=20):\n",
        "    sma = df['close'].rolling(window=window).mean()\n",
        "    std = df['close'].rolling(window=window).std()\n",
        "    upper_band = sma + (std * 2)\n",
        "    lower_band = sma - (std * 2)\n",
        "    return upper_band, lower_band\n",
        "\n",
        "def calculate_macd(df, short_window=12, long_window=26, signal_window=9):\n",
        "    ema_12 = df['close'].ewm(span=short_window, adjust=False).mean()\n",
        "    ema_26 = df['close'].ewm(span=long_window, adjust=False).mean()\n",
        "    macd = ema_12 - ema_26\n",
        "    signal_line = macd.ewm(span=signal_window, adjust=False).mean()\n",
        "    return macd, signal_line\n",
        "\n",
        "def calculate_stochastic_oscillator(df, window=14):\n",
        "    l14 = df['low'].rolling(window=window).min()\n",
        "    h14 = df['high'].rolling(window=window).max()\n",
        "    k = (df['close'] - l14) * 100 / (h14 - l14)\n",
        "    d = k.rolling(window=3).mean()\n",
        "    return k, d\n",
        "\n",
        "def calculate_vwap(df):\n",
        "    cum_tp_volume = (df['close'] * df['volume']).cumsum()\n",
        "    cum_volume = df['volume'].cumsum()\n",
        "    return cum_tp_volume / cum_volume\n",
        "\n",
        "def calculate_atr(df, window=14):\n",
        "    df['H-L'] = df['high'] - df['low']\n",
        "    df['H-PC'] = np.abs(df['high'] - df['close'].shift(1))\n",
        "    df['L-PC'] = np.abs(df['low'] - df['close'].shift(1))\n",
        "    tr = df[['H-L', 'H-PC', 'L-PC']].max(axis=1)\n",
        "    atr = tr.rolling(window=window).mean()\n",
        "    return atr\n",
        "\n",
        "# Add features\n",
        "df = add_features(df)\n",
        "print(\"DataFrame with features:\")\n",
        "print(df.head())\n",
        "\n",
        "if df.empty:\n",
        "    print(\"The DataFrame after adding features is empty. Exiting.\")\n",
        "    exit()\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_size = int(len(df) * 0.8)\n",
        "train_df = df[:train_size]\n",
        "test_df = df[train_size:]\n",
        "\n",
        "X_train = train_df[['SMA_50', 'SMA_200', 'RSI', 'Bollinger_Upper', 'Bollinger_Lower', 'MACD', 'MACD_Signal', 'Stochastic_%K', 'Stochastic_%D', 'Momentum', 'VWAP']]\n",
        "y_train = train_df['Target']\n",
        "\n",
        "X_test = test_df[['SMA_50', 'SMA_200', 'RSI', 'Bollinger_Upper', 'Bollinger_Lower', 'MACD', 'MACD_Signal', 'Stochastic_%K', 'Stochastic_%D', 'Momentum', 'VWAP']]\n",
        "y_test = test_df['Target']\n",
        "\n",
        "print(\"Training set shape:\", X_train.shape)\n",
        "print(\"Test set shape:\", X_test.shape)\n",
        "\n",
        "if X_train.empty or X_test.empty:\n",
        "    print(\"Training or test set is empty. Exiting.\")\n",
        "    exit()\n",
        "\n",
        "# Training Neural Network Model\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import kerastuner as kt\n",
        "\n",
        "# Data Preprocessing\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Hyperparameter Tuning Function\n",
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(units=hp.Int('units_input', min_value=32, max_value=512, step=32), activation='relu', input_dim=X_train_scaled.shape[1]))\n",
        "    model.add(Dropout(hp.Float('dropout_input', 0.0, 0.5, step=0.1)))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    for i in range(hp.Int('num_layers', 1, 3)):\n",
        "        model.add(Dense(units=hp.Int('units_' + str(i), min_value=32, max_value=512, step=32), activation='relu'))\n",
        "        model.add(Dropout(hp.Float('dropout_' + str(i), 0.0, 0.5, step=0.1)))\n",
        "        model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer=Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Hyperparameter Search\n",
        "tuner = kt.RandomSearch(build_model,\n",
        "                        objective='val_accuracy',\n",
        "                        max_trials=10,\n",
        "                        executions_per_trial=1,\n",
        "                        directory='my_dir',\n",
        "                        project_name='intro_to_kt')\n",
        "\n",
        "tuner.search(X_train_scaled, y_train, epochs=10, validation_split=0.1, callbacks=[EarlyStopping(monitor='val_loss', patience=10)])\n",
        "\n",
        "# Get the optimal hyperparameters\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "# Build the model with the optimal hyperparameters\n",
        "nn_model = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "# Check if a pre-trained model exists\n",
        "model_path = 'best_nn_model.h5'\n",
        "if os.path.exists(model_path):\n",
        "    nn_model = load_model(model_path)\n",
        "    print(\"Loaded pre-trained model.\")\n",
        "else:\n",
        "    print(\"Created new model.\")\n",
        "\n",
        "# Continue training the model with early stopping and model checkpoint callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
        "model_checkpoint = ModelCheckpoint(model_path, save_best_only=True, monitor='val_loss')\n",
        "\n",
        "history = nn_model.fit(X_train_scaled, y_train,\n",
        "                       epochs=200,\n",
        "                       batch_size=64,\n",
        "                       validation_split=0.1,\n",
        "                       callbacks=[early_stopping, model_checkpoint],\n",
        "                       verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = nn_model.evaluate(X_test_scaled, y_test)\n",
        "print(f\"Neural Network Model Accuracy: {accuracy}\")\n",
        "\n",
        "# Save Neural Network Model\n",
        "nn_model.save(model_path)\n",
        "\n",
        "# Plot training history\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0, 1])\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dq3BUJQDWMvr",
        "outputId": "e0a7298a-a6ed-4403-a8c8-ab7877f97505"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Login URL: https://kite.zerodha.com/connect/login?api_key=klz728yv89qrljzs&v=3\n",
            "Enter request token: 7D8lKVwjO0Y3M5dFjMC33Tu5HLxoRrz0\n",
            "Data fetched:\n",
            "                       date     open     high      low    close  volume\n",
            "0 2016-01-01 09:15:00+05:30  7938.45  7939.25  7927.35  7927.95       0\n",
            "1 2016-01-01 09:16:00+05:30  7928.65  7929.65  7926.40  7926.45       0\n",
            "2 2016-01-01 09:17:00+05:30  7926.65  7927.70  7922.95  7923.05       0\n",
            "3 2016-01-01 09:18:00+05:30  7923.20  7927.20  7923.20  7926.35       0\n",
            "4 2016-01-01 09:19:00+05:30  7927.00  7927.50  7925.00  7925.05       0\n",
            "SMA_50 added:\n",
            "                             SMA_50\n",
            "date                               \n",
            "2016-01-01 09:15:00+05:30       NaN\n",
            "2016-01-01 09:16:00+05:30       NaN\n",
            "2016-01-01 09:17:00+05:30       NaN\n",
            "2016-01-01 09:18:00+05:30       NaN\n",
            "2016-01-01 09:19:00+05:30       NaN\n",
            "2016-01-01 09:20:00+05:30       NaN\n",
            "2016-01-01 09:21:00+05:30       NaN\n",
            "2016-01-01 09:22:00+05:30       NaN\n",
            "2016-01-01 09:23:00+05:30       NaN\n",
            "2016-01-01 09:24:00+05:30       NaN\n",
            "2016-01-01 09:25:00+05:30       NaN\n",
            "2016-01-01 09:26:00+05:30       NaN\n",
            "2016-01-01 09:27:00+05:30       NaN\n",
            "2016-01-01 09:28:00+05:30       NaN\n",
            "2016-01-01 09:29:00+05:30       NaN\n",
            "2016-01-01 09:30:00+05:30       NaN\n",
            "2016-01-01 09:31:00+05:30       NaN\n",
            "2016-01-01 09:32:00+05:30       NaN\n",
            "2016-01-01 09:33:00+05:30       NaN\n",
            "2016-01-01 09:34:00+05:30       NaN\n",
            "2016-01-01 09:35:00+05:30       NaN\n",
            "2016-01-01 09:36:00+05:30       NaN\n",
            "2016-01-01 09:37:00+05:30       NaN\n",
            "2016-01-01 09:38:00+05:30       NaN\n",
            "2016-01-01 09:39:00+05:30       NaN\n",
            "2016-01-01 09:40:00+05:30       NaN\n",
            "2016-01-01 09:41:00+05:30       NaN\n",
            "2016-01-01 09:42:00+05:30       NaN\n",
            "2016-01-01 09:43:00+05:30       NaN\n",
            "2016-01-01 09:44:00+05:30       NaN\n",
            "2016-01-01 09:45:00+05:30       NaN\n",
            "2016-01-01 09:46:00+05:30       NaN\n",
            "2016-01-01 09:47:00+05:30       NaN\n",
            "2016-01-01 09:48:00+05:30       NaN\n",
            "2016-01-01 09:49:00+05:30       NaN\n",
            "2016-01-01 09:50:00+05:30       NaN\n",
            "2016-01-01 09:51:00+05:30       NaN\n",
            "2016-01-01 09:52:00+05:30       NaN\n",
            "2016-01-01 09:53:00+05:30       NaN\n",
            "2016-01-01 09:54:00+05:30       NaN\n",
            "2016-01-01 09:55:00+05:30       NaN\n",
            "2016-01-01 09:56:00+05:30       NaN\n",
            "2016-01-01 09:57:00+05:30       NaN\n",
            "2016-01-01 09:58:00+05:30       NaN\n",
            "2016-01-01 09:59:00+05:30       NaN\n",
            "2016-01-01 10:00:00+05:30       NaN\n",
            "2016-01-01 10:01:00+05:30       NaN\n",
            "2016-01-01 10:02:00+05:30       NaN\n",
            "2016-01-01 10:03:00+05:30       NaN\n",
            "2016-01-01 10:04:00+05:30  7925.587\n",
            "2016-01-01 10:05:00+05:30  7925.626\n",
            "2016-01-01 10:06:00+05:30  7925.685\n",
            "2016-01-01 10:07:00+05:30  7925.844\n",
            "2016-01-01 10:08:00+05:30  7925.900\n",
            "2016-01-01 10:09:00+05:30  7926.004\n",
            "2016-01-01 10:10:00+05:30  7926.093\n",
            "2016-01-01 10:11:00+05:30  7926.414\n",
            "2016-01-01 10:12:00+05:30  7926.705\n",
            "2016-01-01 10:13:00+05:30  7926.958\n",
            "2016-01-01 10:14:00+05:30  7927.170\n",
            "SMA_200 added:\n",
            "                              SMA_200\n",
            "date                                 \n",
            "2016-01-01 09:15:00+05:30         NaN\n",
            "2016-01-01 09:16:00+05:30         NaN\n",
            "2016-01-01 09:17:00+05:30         NaN\n",
            "2016-01-01 09:18:00+05:30         NaN\n",
            "2016-01-01 09:19:00+05:30         NaN\n",
            "...                               ...\n",
            "2016-01-01 12:40:00+05:30  7937.78825\n",
            "2016-01-01 12:41:00+05:30  7937.90675\n",
            "2016-01-01 12:42:00+05:30  7938.01025\n",
            "2016-01-01 12:43:00+05:30  7938.09350\n",
            "2016-01-01 12:44:00+05:30  7938.15450\n",
            "\n",
            "[210 rows x 1 columns]\n",
            "RSI added:\n",
            "                                 RSI\n",
            "date                                \n",
            "2016-01-01 09:15:00+05:30        NaN\n",
            "2016-01-01 09:16:00+05:30        NaN\n",
            "2016-01-01 09:17:00+05:30        NaN\n",
            "2016-01-01 09:18:00+05:30        NaN\n",
            "2016-01-01 09:19:00+05:30        NaN\n",
            "2016-01-01 09:20:00+05:30        NaN\n",
            "2016-01-01 09:21:00+05:30        NaN\n",
            "2016-01-01 09:22:00+05:30        NaN\n",
            "2016-01-01 09:23:00+05:30        NaN\n",
            "2016-01-01 09:24:00+05:30        NaN\n",
            "2016-01-01 09:25:00+05:30        NaN\n",
            "2016-01-01 09:26:00+05:30        NaN\n",
            "2016-01-01 09:27:00+05:30        NaN\n",
            "2016-01-01 09:28:00+05:30  45.721925\n",
            "2016-01-01 09:29:00+05:30  42.379182\n",
            "2016-01-01 09:30:00+05:30  42.118227\n",
            "2016-01-01 09:31:00+05:30  46.112601\n",
            "2016-01-01 09:32:00+05:30  44.931507\n",
            "2016-01-01 09:33:00+05:30  45.054945\n",
            "2016-01-01 09:34:00+05:30  51.807229\n",
            "Bollinger Bands added:\n",
            "                           Bollinger_Upper  Bollinger_Lower\n",
            "date                                                       \n",
            "2016-01-01 09:15:00+05:30              NaN              NaN\n",
            "2016-01-01 09:16:00+05:30              NaN              NaN\n",
            "2016-01-01 09:17:00+05:30              NaN              NaN\n",
            "2016-01-01 09:18:00+05:30              NaN              NaN\n",
            "2016-01-01 09:19:00+05:30              NaN              NaN\n",
            "2016-01-01 09:20:00+05:30              NaN              NaN\n",
            "2016-01-01 09:21:00+05:30              NaN              NaN\n",
            "2016-01-01 09:22:00+05:30              NaN              NaN\n",
            "2016-01-01 09:23:00+05:30              NaN              NaN\n",
            "2016-01-01 09:24:00+05:30              NaN              NaN\n",
            "2016-01-01 09:25:00+05:30              NaN              NaN\n",
            "2016-01-01 09:26:00+05:30              NaN              NaN\n",
            "2016-01-01 09:27:00+05:30              NaN              NaN\n",
            "2016-01-01 09:28:00+05:30              NaN              NaN\n",
            "2016-01-01 09:29:00+05:30              NaN              NaN\n",
            "2016-01-01 09:30:00+05:30              NaN              NaN\n",
            "2016-01-01 09:31:00+05:30              NaN              NaN\n",
            "2016-01-01 09:32:00+05:30              NaN              NaN\n",
            "2016-01-01 09:33:00+05:30              NaN              NaN\n",
            "2016-01-01 09:34:00+05:30      7930.893091      7914.426909\n",
            "2016-01-01 09:35:00+05:30      7930.561136      7914.553864\n",
            "2016-01-01 09:36:00+05:30      7930.599970      7914.540030\n",
            "2016-01-01 09:37:00+05:30      7930.839753      7914.555247\n",
            "2016-01-01 09:38:00+05:30      7930.501358      7914.573642\n",
            "2016-01-01 09:39:00+05:30      7930.264640      7914.510360\n",
            "MACD added:\n",
            "                               MACD  MACD_Signal\n",
            "date                                            \n",
            "2016-01-01 09:15:00+05:30  0.000000     0.000000\n",
            "2016-01-01 09:16:00+05:30 -0.119658    -0.023932\n",
            "2016-01-01 09:17:00+05:30 -0.483269    -0.115799\n",
            "2016-01-01 09:18:00+05:30 -0.499394    -0.192518\n",
            "2016-01-01 09:19:00+05:30 -0.610040    -0.276022\n",
            "2016-01-01 09:20:00+05:30 -0.677811    -0.356380\n",
            "2016-01-01 09:21:00+05:30 -1.680448    -0.621194\n",
            "2016-01-01 09:22:00+05:30 -2.303250    -0.957605\n",
            "2016-01-01 09:23:00+05:30 -2.633328    -1.292750\n",
            "2016-01-01 09:24:00+05:30 -2.562782    -1.546756\n",
            "2016-01-01 09:25:00+05:30 -2.211069    -1.679619\n",
            "2016-01-01 09:26:00+05:30 -1.750768    -1.693849\n",
            "2016-01-01 09:27:00+05:30 -1.282433    -1.611566\n",
            "2016-01-01 09:28:00+05:30 -1.068411    -1.502935\n",
            "2016-01-01 09:29:00+05:30 -1.123881    -1.427124\n",
            "2016-01-01 09:30:00+05:30 -1.294134    -1.400526\n",
            "2016-01-01 09:31:00+05:30 -1.404798    -1.401380\n",
            "2016-01-01 09:32:00+05:30 -1.276062    -1.376317\n",
            "2016-01-01 09:33:00+05:30 -1.256384    -1.352330\n",
            "2016-01-01 09:34:00+05:30 -0.807846    -1.243433\n",
            "2016-01-01 09:35:00+05:30 -0.511039    -1.096954\n",
            "2016-01-01 09:36:00+05:30 -0.208856    -0.919335\n",
            "2016-01-01 09:37:00+05:30 -0.057472    -0.746962\n",
            "2016-01-01 09:38:00+05:30 -0.133653    -0.624300\n",
            "2016-01-01 09:39:00+05:30 -0.279566    -0.555353\n",
            "2016-01-01 09:40:00+05:30 -0.338847    -0.512052\n",
            "2016-01-01 09:41:00+05:30 -0.217897    -0.453221\n",
            "2016-01-01 09:42:00+05:30 -0.164528    -0.395483\n",
            "2016-01-01 09:43:00+05:30 -0.088931    -0.334172\n",
            "2016-01-01 09:44:00+05:30 -0.108461    -0.289030\n",
            "Stochastic Oscillator added:\n",
            "                           Stochastic_%K  Stochastic_%D\n",
            "date                                                   \n",
            "2016-01-01 09:15:00+05:30            NaN            NaN\n",
            "2016-01-01 09:16:00+05:30            NaN            NaN\n",
            "2016-01-01 09:17:00+05:30            NaN            NaN\n",
            "2016-01-01 09:18:00+05:30            NaN            NaN\n",
            "2016-01-01 09:19:00+05:30            NaN            NaN\n",
            "2016-01-01 09:20:00+05:30            NaN            NaN\n",
            "2016-01-01 09:21:00+05:30            NaN            NaN\n",
            "2016-01-01 09:22:00+05:30            NaN            NaN\n",
            "2016-01-01 09:23:00+05:30            NaN            NaN\n",
            "2016-01-01 09:24:00+05:30            NaN            NaN\n",
            "2016-01-01 09:25:00+05:30            NaN            NaN\n",
            "2016-01-01 09:26:00+05:30            NaN            NaN\n",
            "2016-01-01 09:27:00+05:30            NaN            NaN\n",
            "2016-01-01 09:28:00+05:30      50.511945            NaN\n",
            "2016-01-01 09:29:00+05:30      60.152284            NaN\n",
            "2016-01-01 09:30:00+05:30      55.647383      55.437204\n",
            "2016-01-01 09:31:00+05:30      56.198347      57.332671\n",
            "2016-01-01 09:32:00+05:30      69.972452      60.606061\n",
            "2016-01-01 09:33:00+05:30      63.360882      63.177227\n",
            "2016-01-01 09:34:00+05:30      92.286501      75.206612\n",
            "Momentum added:\n",
            "                           Momentum\n",
            "date                               \n",
            "2016-01-01 09:15:00+05:30       NaN\n",
            "2016-01-01 09:16:00+05:30       NaN\n",
            "2016-01-01 09:17:00+05:30       NaN\n",
            "2016-01-01 09:18:00+05:30       NaN\n",
            "2016-01-01 09:19:00+05:30       NaN\n",
            "2016-01-01 09:20:00+05:30       NaN\n",
            "2016-01-01 09:21:00+05:30       NaN\n",
            "2016-01-01 09:22:00+05:30       NaN\n",
            "2016-01-01 09:23:00+05:30       NaN\n",
            "2016-01-01 09:24:00+05:30       NaN\n",
            "2016-01-01 09:25:00+05:30 -0.000530\n",
            "2016-01-01 09:26:00+05:30 -0.000088\n",
            "2016-01-01 09:27:00+05:30  0.000480\n",
            "2016-01-01 09:28:00+05:30 -0.000202\n",
            "2016-01-01 09:29:00+05:30 -0.000410\n",
            "VWAP added:\n",
            "                           VWAP\n",
            "date                           \n",
            "2016-01-01 09:15:00+05:30   NaN\n",
            "2016-01-01 09:16:00+05:30   NaN\n",
            "2016-01-01 09:17:00+05:30   NaN\n",
            "2016-01-01 09:18:00+05:30   NaN\n",
            "2016-01-01 09:19:00+05:30   NaN\n",
            "2016-01-01 09:20:00+05:30   NaN\n",
            "2016-01-01 09:21:00+05:30   NaN\n",
            "2016-01-01 09:22:00+05:30   NaN\n",
            "2016-01-01 09:23:00+05:30   NaN\n",
            "2016-01-01 09:24:00+05:30   NaN\n",
            "2016-01-01 09:25:00+05:30   NaN\n",
            "2016-01-01 09:26:00+05:30   NaN\n",
            "2016-01-01 09:27:00+05:30   NaN\n",
            "2016-01-01 09:28:00+05:30   NaN\n",
            "2016-01-01 09:29:00+05:30   NaN\n",
            "ATR added:\n",
            "                            ATR\n",
            "date                           \n",
            "2016-01-01 09:15:00+05:30   NaN\n",
            "2016-01-01 09:16:00+05:30   NaN\n",
            "2016-01-01 09:17:00+05:30   NaN\n",
            "2016-01-01 09:18:00+05:30   NaN\n",
            "2016-01-01 09:19:00+05:30   NaN\n",
            "2016-01-01 09:20:00+05:30   NaN\n",
            "2016-01-01 09:21:00+05:30   NaN\n",
            "2016-01-01 09:22:00+05:30   NaN\n",
            "2016-01-01 09:23:00+05:30   NaN\n",
            "2016-01-01 09:24:00+05:30   NaN\n",
            "2016-01-01 09:25:00+05:30   NaN\n",
            "2016-01-01 09:26:00+05:30   NaN\n",
            "2016-01-01 09:27:00+05:30   NaN\n",
            "2016-01-01 09:28:00+05:30  4.95\n",
            "2016-01-01 09:29:00+05:30  4.40\n",
            "Target added:\n",
            "                           Target\n",
            "date                             \n",
            "2016-01-01 09:15:00+05:30       0\n",
            "2016-01-01 09:16:00+05:30       0\n",
            "2016-01-01 09:17:00+05:30       1\n",
            "2016-01-01 09:18:00+05:30       0\n",
            "2016-01-01 09:19:00+05:30       1\n",
            "2016-01-01 09:20:00+05:30       0\n",
            "2016-01-01 09:21:00+05:30       1\n",
            "2016-01-01 09:22:00+05:30       1\n",
            "2016-01-01 09:23:00+05:30       1\n",
            "2016-01-01 09:24:00+05:30       1\n",
            "2016-01-01 09:25:00+05:30       1\n",
            "2016-01-01 09:26:00+05:30       1\n",
            "2016-01-01 09:27:00+05:30       0\n",
            "2016-01-01 09:28:00+05:30       0\n",
            "2016-01-01 09:29:00+05:30       0\n",
            "DataFrame before dropping NaNs:\n",
            "                              open     high      low    close  volume  \\\n",
            "date                                                                    \n",
            "2016-01-01 09:15:00+05:30  7938.45  7939.25  7927.35  7927.95       0   \n",
            "2016-01-01 09:16:00+05:30  7928.65  7929.65  7926.40  7926.45       0   \n",
            "2016-01-01 09:17:00+05:30  7926.65  7927.70  7922.95  7923.05       0   \n",
            "2016-01-01 09:18:00+05:30  7923.20  7927.20  7923.20  7926.35       0   \n",
            "2016-01-01 09:19:00+05:30  7927.00  7927.50  7925.00  7925.05       0   \n",
            "...                            ...      ...      ...      ...     ...   \n",
            "2016-01-01 12:50:00+05:30  7935.25  7936.35  7934.75  7936.00       0   \n",
            "2016-01-01 12:51:00+05:30  7936.15  7936.95  7935.05  7936.60       0   \n",
            "2016-01-01 12:52:00+05:30  7936.65  7936.80  7936.20  7936.40       0   \n",
            "2016-01-01 12:53:00+05:30  7937.15  7937.60  7936.35  7937.25       0   \n",
            "2016-01-01 12:54:00+05:30  7937.30  7937.30  7936.05  7936.75       0   \n",
            "\n",
            "                             SMA_50     SMA_200        RSI  Bollinger_Upper  \\\n",
            "date                                                                          \n",
            "2016-01-01 09:15:00+05:30       NaN         NaN        NaN              NaN   \n",
            "2016-01-01 09:16:00+05:30       NaN         NaN        NaN              NaN   \n",
            "2016-01-01 09:17:00+05:30       NaN         NaN        NaN              NaN   \n",
            "2016-01-01 09:18:00+05:30       NaN         NaN        NaN              NaN   \n",
            "2016-01-01 09:19:00+05:30       NaN         NaN        NaN              NaN   \n",
            "...                             ...         ...        ...              ...   \n",
            "2016-01-01 12:50:00+05:30  7940.065  7938.47250  34.334764      7942.613425   \n",
            "2016-01-01 12:51:00+05:30  7939.877  7938.55475  37.860082      7942.305619   \n",
            "2016-01-01 12:52:00+05:30  7939.674  7938.62350  39.655172      7941.749161   \n",
            "2016-01-01 12:53:00+05:30  7939.492  7938.70250  49.099099      7941.187916   \n",
            "2016-01-01 12:54:00+05:30  7939.302  7938.75275  52.153110      7940.531860   \n",
            "\n",
            "                           Bollinger_Lower  ...  MACD_Signal  Stochastic_%K  \\\n",
            "date                                        ...                               \n",
            "2016-01-01 09:15:00+05:30              NaN  ...     0.000000            NaN   \n",
            "2016-01-01 09:16:00+05:30              NaN  ...    -0.023932            NaN   \n",
            "2016-01-01 09:17:00+05:30              NaN  ...    -0.115799            NaN   \n",
            "2016-01-01 09:18:00+05:30              NaN  ...    -0.192518            NaN   \n",
            "2016-01-01 09:19:00+05:30              NaN  ...    -0.276022            NaN   \n",
            "...                                    ...  ...          ...            ...   \n",
            "2016-01-01 12:50:00+05:30      7931.206575  ...    -1.627068      51.234568   \n",
            "2016-01-01 12:51:00+05:30      7931.204381  ...    -1.565968      60.126582   \n",
            "2016-01-01 12:52:00+05:30      7931.320839  ...    -1.484401      64.084507   \n",
            "2016-01-01 12:53:00+05:30      7931.527084  ...    -1.377338      87.096774   \n",
            "2016-01-01 12:54:00+05:30      7931.783140  ...    -1.264744      85.217391   \n",
            "\n",
            "                           Stochastic_%D  Momentum  VWAP    H-L  H-PC  L-PC  \\\n",
            "date                                                                          \n",
            "2016-01-01 09:15:00+05:30            NaN       NaN   NaN  11.90   NaN   NaN   \n",
            "2016-01-01 09:16:00+05:30            NaN       NaN   NaN   3.25  1.70  1.55   \n",
            "2016-01-01 09:17:00+05:30            NaN       NaN   NaN   4.75  1.25  3.50   \n",
            "2016-01-01 09:18:00+05:30            NaN       NaN   NaN   4.00  4.15  0.15   \n",
            "2016-01-01 09:19:00+05:30            NaN       NaN   NaN   2.50  1.15  1.35   \n",
            "...                                  ...       ...   ...    ...   ...   ...   \n",
            "2016-01-01 12:50:00+05:30      41.152263 -0.000038   NaN   1.60  1.35  0.25   \n",
            "2016-01-01 12:51:00+05:30      50.083346 -0.000038   NaN   1.90  0.95  0.95   \n",
            "2016-01-01 12:52:00+05:30      58.481886  0.000088   NaN   0.60  0.20  0.40   \n",
            "2016-01-01 12:53:00+05:30      70.435955  0.000498   NaN   1.25  1.20  0.05   \n",
            "2016-01-01 12:54:00+05:30      78.799558  0.000523   NaN   1.25  0.05  1.20   \n",
            "\n",
            "                                ATR  Target  \n",
            "date                                         \n",
            "2016-01-01 09:15:00+05:30       NaN       0  \n",
            "2016-01-01 09:16:00+05:30       NaN       0  \n",
            "2016-01-01 09:17:00+05:30       NaN       1  \n",
            "2016-01-01 09:18:00+05:30       NaN       0  \n",
            "2016-01-01 09:19:00+05:30       NaN       1  \n",
            "...                             ...     ...  \n",
            "2016-01-01 12:50:00+05:30  1.550000       1  \n",
            "2016-01-01 12:51:00+05:30  1.639286       0  \n",
            "2016-01-01 12:52:00+05:30  1.539286       1  \n",
            "2016-01-01 12:53:00+05:30  1.521429       0  \n",
            "2016-01-01 12:54:00+05:30  1.453571       0  \n",
            "\n",
            "[220 rows x 21 columns]\n",
            "DataFrame after dropping NaNs:\n",
            "                              open     high      low    close  volume  \\\n",
            "date                                                                    \n",
            "2016-01-01 09:15:00+05:30  7938.45  7939.25  7927.35  7927.95       0   \n",
            "2016-01-01 09:16:00+05:30  7928.65  7929.65  7926.40  7926.45       0   \n",
            "2016-01-01 09:17:00+05:30  7926.65  7927.70  7922.95  7923.05       0   \n",
            "2016-01-01 09:18:00+05:30  7923.20  7927.20  7923.20  7926.35       0   \n",
            "2016-01-01 09:19:00+05:30  7927.00  7927.50  7925.00  7925.05       0   \n",
            "...                            ...      ...      ...      ...     ...   \n",
            "2016-01-01 12:50:00+05:30  7935.25  7936.35  7934.75  7936.00       0   \n",
            "2016-01-01 12:51:00+05:30  7936.15  7936.95  7935.05  7936.60       0   \n",
            "2016-01-01 12:52:00+05:30  7936.65  7936.80  7936.20  7936.40       0   \n",
            "2016-01-01 12:53:00+05:30  7937.15  7937.60  7936.35  7937.25       0   \n",
            "2016-01-01 12:54:00+05:30  7937.30  7937.30  7936.05  7936.75       0   \n",
            "\n",
            "                             SMA_50     SMA_200        RSI  Bollinger_Upper  \\\n",
            "date                                                                          \n",
            "2016-01-01 09:15:00+05:30       NaN         NaN        NaN              NaN   \n",
            "2016-01-01 09:16:00+05:30       NaN         NaN        NaN              NaN   \n",
            "2016-01-01 09:17:00+05:30       NaN         NaN        NaN              NaN   \n",
            "2016-01-01 09:18:00+05:30       NaN         NaN        NaN              NaN   \n",
            "2016-01-01 09:19:00+05:30       NaN         NaN        NaN              NaN   \n",
            "...                             ...         ...        ...              ...   \n",
            "2016-01-01 12:50:00+05:30  7940.065  7938.47250  34.334764      7942.613425   \n",
            "2016-01-01 12:51:00+05:30  7939.877  7938.55475  37.860082      7942.305619   \n",
            "2016-01-01 12:52:00+05:30  7939.674  7938.62350  39.655172      7941.749161   \n",
            "2016-01-01 12:53:00+05:30  7939.492  7938.70250  49.099099      7941.187916   \n",
            "2016-01-01 12:54:00+05:30  7939.302  7938.75275  52.153110      7940.531860   \n",
            "\n",
            "                           Bollinger_Lower  ...  MACD_Signal  Stochastic_%K  \\\n",
            "date                                        ...                               \n",
            "2016-01-01 09:15:00+05:30              NaN  ...     0.000000            NaN   \n",
            "2016-01-01 09:16:00+05:30              NaN  ...    -0.023932            NaN   \n",
            "2016-01-01 09:17:00+05:30              NaN  ...    -0.115799            NaN   \n",
            "2016-01-01 09:18:00+05:30              NaN  ...    -0.192518            NaN   \n",
            "2016-01-01 09:19:00+05:30              NaN  ...    -0.276022            NaN   \n",
            "...                                    ...  ...          ...            ...   \n",
            "2016-01-01 12:50:00+05:30      7931.206575  ...    -1.627068      51.234568   \n",
            "2016-01-01 12:51:00+05:30      7931.204381  ...    -1.565968      60.126582   \n",
            "2016-01-01 12:52:00+05:30      7931.320839  ...    -1.484401      64.084507   \n",
            "2016-01-01 12:53:00+05:30      7931.527084  ...    -1.377338      87.096774   \n",
            "2016-01-01 12:54:00+05:30      7931.783140  ...    -1.264744      85.217391   \n",
            "\n",
            "                           Stochastic_%D  Momentum  VWAP    H-L  H-PC  L-PC  \\\n",
            "date                                                                          \n",
            "2016-01-01 09:15:00+05:30            NaN       NaN   NaN  11.90   NaN   NaN   \n",
            "2016-01-01 09:16:00+05:30            NaN       NaN   NaN   3.25  1.70  1.55   \n",
            "2016-01-01 09:17:00+05:30            NaN       NaN   NaN   4.75  1.25  3.50   \n",
            "2016-01-01 09:18:00+05:30            NaN       NaN   NaN   4.00  4.15  0.15   \n",
            "2016-01-01 09:19:00+05:30            NaN       NaN   NaN   2.50  1.15  1.35   \n",
            "...                                  ...       ...   ...    ...   ...   ...   \n",
            "2016-01-01 12:50:00+05:30      41.152263 -0.000038   NaN   1.60  1.35  0.25   \n",
            "2016-01-01 12:51:00+05:30      50.083346 -0.000038   NaN   1.90  0.95  0.95   \n",
            "2016-01-01 12:52:00+05:30      58.481886  0.000088   NaN   0.60  0.20  0.40   \n",
            "2016-01-01 12:53:00+05:30      70.435955  0.000498   NaN   1.25  1.20  0.05   \n",
            "2016-01-01 12:54:00+05:30      78.799558  0.000523   NaN   1.25  0.05  1.20   \n",
            "\n",
            "                                ATR  Target  \n",
            "date                                         \n",
            "2016-01-01 09:15:00+05:30       NaN       0  \n",
            "2016-01-01 09:16:00+05:30       NaN       0  \n",
            "2016-01-01 09:17:00+05:30       NaN       1  \n",
            "2016-01-01 09:18:00+05:30       NaN       0  \n",
            "2016-01-01 09:19:00+05:30       NaN       1  \n",
            "...                             ...     ...  \n",
            "2016-01-01 12:50:00+05:30  1.550000       1  \n",
            "2016-01-01 12:51:00+05:30  1.639286       0  \n",
            "2016-01-01 12:52:00+05:30  1.539286       1  \n",
            "2016-01-01 12:53:00+05:30  1.521429       0  \n",
            "2016-01-01 12:54:00+05:30  1.453571       0  \n",
            "\n",
            "[220 rows x 21 columns]\n",
            "DataFrame with features:\n",
            "                              open     high      low    close  volume  SMA_50  \\\n",
            "date                                                                            \n",
            "2016-01-01 09:15:00+05:30  7938.45  7939.25  7927.35  7927.95       0     NaN   \n",
            "2016-01-01 09:16:00+05:30  7928.65  7929.65  7926.40  7926.45       0     NaN   \n",
            "2016-01-01 09:17:00+05:30  7926.65  7927.70  7922.95  7923.05       0     NaN   \n",
            "2016-01-01 09:18:00+05:30  7923.20  7927.20  7923.20  7926.35       0     NaN   \n",
            "2016-01-01 09:19:00+05:30  7927.00  7927.50  7925.00  7925.05       0     NaN   \n",
            "\n",
            "                           SMA_200  RSI  Bollinger_Upper  Bollinger_Lower  \\\n",
            "date                                                                        \n",
            "2016-01-01 09:15:00+05:30      NaN  NaN              NaN              NaN   \n",
            "2016-01-01 09:16:00+05:30      NaN  NaN              NaN              NaN   \n",
            "2016-01-01 09:17:00+05:30      NaN  NaN              NaN              NaN   \n",
            "2016-01-01 09:18:00+05:30      NaN  NaN              NaN              NaN   \n",
            "2016-01-01 09:19:00+05:30      NaN  NaN              NaN              NaN   \n",
            "\n",
            "                           ...  MACD_Signal  Stochastic_%K  Stochastic_%D  \\\n",
            "date                       ...                                              \n",
            "2016-01-01 09:15:00+05:30  ...     0.000000            NaN            NaN   \n",
            "2016-01-01 09:16:00+05:30  ...    -0.023932            NaN            NaN   \n",
            "2016-01-01 09:17:00+05:30  ...    -0.115799            NaN            NaN   \n",
            "2016-01-01 09:18:00+05:30  ...    -0.192518            NaN            NaN   \n",
            "2016-01-01 09:19:00+05:30  ...    -0.276022            NaN            NaN   \n",
            "\n",
            "                           Momentum  VWAP    H-L  H-PC  L-PC  ATR  Target  \n",
            "date                                                                       \n",
            "2016-01-01 09:15:00+05:30       NaN   NaN  11.90   NaN   NaN  NaN       0  \n",
            "2016-01-01 09:16:00+05:30       NaN   NaN   3.25  1.70  1.55  NaN       0  \n",
            "2016-01-01 09:17:00+05:30       NaN   NaN   4.75  1.25  3.50  NaN       1  \n",
            "2016-01-01 09:18:00+05:30       NaN   NaN   4.00  4.15  0.15  NaN       0  \n",
            "2016-01-01 09:19:00+05:30       NaN   NaN   2.50  1.15  1.35  NaN       1  \n",
            "\n",
            "[5 rows x 21 columns]\n",
            "Training set shape: (582628, 11)\n",
            "Test set shape: (145657, 11)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-2b7f4651fb10>:187: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
            "  import kerastuner as kt\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/extmath.py:1047: RuntimeWarning: invalid value encountered in divide\n",
            "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/extmath.py:1052: RuntimeWarning: invalid value encountered in divide\n",
            "  T = new_sum / new_sample_count\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/extmath.py:1072: RuntimeWarning: invalid value encountered in divide\n",
            "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Search: Running Trial #1\n",
            "\n",
            "Value             |Best Value So Far |Hyperparameter\n",
            "448               |448               |units_input\n",
            "0.1               |0.1               |dropout_input\n",
            "1                 |1                 |num_layers\n",
            "224               |224               |units_0\n",
            "0.4               |0.4               |dropout_0\n",
            "0.0001            |0.0001            |learning_rate\n",
            "\n",
            "Epoch 1/10\n",
            "16387/16387 [==============================] - 116s 7ms/step - loss: nan - accuracy: 0.5019 - val_loss: nan - val_accuracy: 0.5045\n",
            "Epoch 2/10\n",
            "16387/16387 [==============================] - 106s 6ms/step - loss: nan - accuracy: 0.5019 - val_loss: nan - val_accuracy: 0.5045\n",
            "Epoch 3/10\n",
            "16387/16387 [==============================] - 112s 7ms/step - loss: nan - accuracy: 0.5019 - val_loss: nan - val_accuracy: 0.5045\n",
            "Epoch 4/10\n",
            "16387/16387 [==============================] - 108s 7ms/step - loss: nan - accuracy: 0.5019 - val_loss: nan - val_accuracy: 0.5045\n",
            "Epoch 5/10\n",
            "16387/16387 [==============================] - 117s 7ms/step - loss: nan - accuracy: 0.5019 - val_loss: nan - val_accuracy: 0.5045\n",
            "Epoch 6/10\n",
            "16387/16387 [==============================] - 109s 7ms/step - loss: nan - accuracy: 0.5019 - val_loss: nan - val_accuracy: 0.5045\n",
            "Epoch 7/10\n",
            "16387/16387 [==============================] - 105s 6ms/step - loss: nan - accuracy: 0.5019 - val_loss: nan - val_accuracy: 0.5045\n",
            "Epoch 8/10\n",
            "16387/16387 [==============================] - 107s 7ms/step - loss: nan - accuracy: 0.5019 - val_loss: nan - val_accuracy: 0.5045\n",
            "Epoch 9/10\n",
            " 6117/16387 [==========>...................] - ETA: 1:07 - loss: nan - accuracy: 0.5017"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-2b7f4651fb10>\u001b[0m in \u001b[0;36m<cell line: 221>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    219\u001b[0m                         project_name='intro_to_kt')\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;31m# Get the optimal hyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/base_tuner.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_run_and_update_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_search_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/base_tuner.py\u001b[0m in \u001b[0;36m_try_run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_run_and_update_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_and_update_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrial_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOMPLETED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/base_tuner.py\u001b[0m in \u001b[0;36m_run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_and_update_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m         if self.oracle.get_trial(trial.trial_id).metrics.exists(\n\u001b[1;32m    241\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/tuner.py\u001b[0m in \u001b[0;36mrun_trial\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mcopied_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"callbacks\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mobj_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_and_fit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcopied_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0mhistories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/tuner.py\u001b[0m in \u001b[0;36m_build_and_fit_model\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0mhp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;31m# Save the build config for model loading later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/hypermodel.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, hp, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0mIf\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m \u001b[0mshould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \"\"\"\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[['SMA_50', 'SMA_200', 'RSI', 'Bollinger_Upper', 'Bollinger_Lower', 'MACD', 'MACD_Signal', 'Stochastic_%K', 'Stochastic_%D', 'Momentum', 'VWAP']]\n",
        "y = df['Target']"
      ],
      "metadata": {
        "id": "jlmsfOF-i3_M"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Random Forest Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = rf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Random Forest Model Accuracy: {accuracy}\")\n",
        "\n",
        "# Save Random Forest Model\n",
        "import joblib\n",
        "joblib.dump(rf, 'best_rf_model.joblib')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CP0nfTr0WvQ5",
        "outputId": "26a05b53-7fe7-4e5d-bae9-e7d8d83e7629"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Model Accuracy: 0.5473940327026042\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['best_rf_model.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Neural Network Model\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import kerastuner as kt\n",
        "\n",
        "# Data Preprocessing\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Hyperparameter Tuning Function\n",
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(units=hp.Int('units_input', min_value=32, max_value=512, step=32), activation='relu', input_dim=X_train_scaled.shape[1]))\n",
        "    model.add(Dropout(hp.Float('dropout_input', 0.0, 0.5, step=0.1)))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    for i in range(hp.Int('num_layers', 1, 3)):\n",
        "        model.add(Dense(units=hp.Int('units_' + str(i), min_value=32, max_value=512, step=32), activation='relu'))\n",
        "        model.add(Dropout(hp.Float('dropout_' + str(i), 0.0, 0.5, step=0.1)))\n",
        "        model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer=Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Hyperparameter Search\n",
        "tuner = kt.RandomSearch(build_model,\n",
        "                        objective='val_accuracy',\n",
        "                        max_trials=10,\n",
        "                        executions_per_trial=1,\n",
        "                        directory='my_dir',\n",
        "                        project_name='intro_to_kt')\n",
        "\n",
        "tuner.search(X_train_scaled, y_train, epochs=10, validation_split=0.1, callbacks=[EarlyStopping(monitor='val_loss', patience=10)])\n",
        "\n",
        "# Get the optimal hyperparameters\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "# Build the model with the optimal hyperparameters\n",
        "nn_model = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "# Check if a pre-trained model exists\n",
        "model_path = 'best_nn_model.h5'\n",
        "if os.path.exists(model_path):\n",
        "    nn_model = load_model(model_path)\n",
        "    print(\"Loaded pre-trained model.\")\n",
        "else:\n",
        "    print(\"Created new model.\")\n",
        "\n",
        "# Continue training the model with early stopping and model checkpoint callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
        "model_checkpoint = ModelCheckpoint(model_path, save_best_only=True, monitor='val_loss')\n",
        "\n",
        "history = nn_model.fit(X_train_scaled, y_train,\n",
        "                       epochs=200,\n",
        "                       batch_size=64,\n",
        "                       validation_split=0.1,\n",
        "                       callbacks=[early_stopping, model_checkpoint],\n",
        "                       verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = nn_model.evaluate(X_test_scaled, y_test)\n",
        "print(f\"Neural Network Model Accuracy: {accuracy}\")\n",
        "\n",
        "# Save Neural Network Model\n",
        "nn_model.save(model_path)\n",
        "\n",
        "\n",
        "\n",
        "# Plot training history\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0, 1])\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nYeJyjniWyfS",
        "outputId": "2d880143-2351-4282-bfd4-ab118d98b3ad"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 10 Complete [00h 15m 07s]\n",
            "val_accuracy: 0.5478023290634155\n",
            "\n",
            "Best val_accuracy So Far: 0.5486099123954773\n",
            "Total elapsed time: 04h 02m 18s\n",
            "Loaded pre-trained model.\n",
            "Epoch 1/200\n",
            "8184/8184 [==============================] - 23s 3ms/step - loss: 0.6861 - accuracy: 0.5474 - val_loss: 0.6859 - val_accuracy: 0.5496\n",
            "Epoch 2/200\n",
            "  66/8184 [..............................] - ETA: 19s - loss: 0.6883 - accuracy: 0.5367"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8184/8184 [==============================] - 27s 3ms/step - loss: 0.6860 - accuracy: 0.5477 - val_loss: 0.6860 - val_accuracy: 0.5493\n",
            "Epoch 3/200\n",
            "8184/8184 [==============================] - 22s 3ms/step - loss: 0.6861 - accuracy: 0.5471 - val_loss: 0.6861 - val_accuracy: 0.5484\n",
            "Epoch 4/200\n",
            "8184/8184 [==============================] - 23s 3ms/step - loss: 0.6860 - accuracy: 0.5476 - val_loss: 0.6860 - val_accuracy: 0.5490\n",
            "Epoch 5/200\n",
            "8184/8184 [==============================] - 20s 2ms/step - loss: 0.6860 - accuracy: 0.5477 - val_loss: 0.6861 - val_accuracy: 0.5490\n",
            "Epoch 6/200\n",
            "8184/8184 [==============================] - 22s 3ms/step - loss: 0.6860 - accuracy: 0.5478 - val_loss: 0.6862 - val_accuracy: 0.5493\n",
            "Epoch 7/200\n",
            "8184/8184 [==============================] - 20s 2ms/step - loss: 0.6860 - accuracy: 0.5473 - val_loss: 0.6860 - val_accuracy: 0.5491\n",
            "Epoch 8/200\n",
            "8184/8184 [==============================] - 22s 3ms/step - loss: 0.6860 - accuracy: 0.5476 - val_loss: 0.6861 - val_accuracy: 0.5478\n",
            "Epoch 9/200\n",
            "8184/8184 [==============================] - 20s 2ms/step - loss: 0.6860 - accuracy: 0.5475 - val_loss: 0.6861 - val_accuracy: 0.5479\n",
            "Epoch 10/200\n",
            "8184/8184 [==============================] - 22s 3ms/step - loss: 0.6860 - accuracy: 0.5477 - val_loss: 0.6859 - val_accuracy: 0.5486\n",
            "Epoch 11/200\n",
            "8184/8184 [==============================] - 21s 3ms/step - loss: 0.6860 - accuracy: 0.5481 - val_loss: 0.6861 - val_accuracy: 0.5489\n",
            "Epoch 12/200\n",
            "8184/8184 [==============================] - 24s 3ms/step - loss: 0.6860 - accuracy: 0.5477 - val_loss: 0.6860 - val_accuracy: 0.5488\n",
            "Epoch 13/200\n",
            "8184/8184 [==============================] - 30s 4ms/step - loss: 0.6860 - accuracy: 0.5480 - val_loss: 0.6861 - val_accuracy: 0.5493\n",
            "Epoch 14/200\n",
            "8184/8184 [==============================] - 27s 3ms/step - loss: 0.6860 - accuracy: 0.5478 - val_loss: 0.6861 - val_accuracy: 0.5488\n",
            "Epoch 15/200\n",
            "8184/8184 [==============================] - 26s 3ms/step - loss: 0.6860 - accuracy: 0.5479 - val_loss: 0.6861 - val_accuracy: 0.5484\n",
            "Epoch 16/200\n",
            "8184/8184 [==============================] - 25s 3ms/step - loss: 0.6859 - accuracy: 0.5481 - val_loss: 0.6860 - val_accuracy: 0.5489\n",
            "Epoch 17/200\n",
            "8184/8184 [==============================] - 25s 3ms/step - loss: 0.6859 - accuracy: 0.5478 - val_loss: 0.6862 - val_accuracy: 0.5476\n",
            "Epoch 18/200\n",
            "8184/8184 [==============================] - 25s 3ms/step - loss: 0.6859 - accuracy: 0.5479 - val_loss: 0.6861 - val_accuracy: 0.5485\n",
            "Epoch 19/200\n",
            "8184/8184 [==============================] - 25s 3ms/step - loss: 0.6860 - accuracy: 0.5478 - val_loss: 0.6860 - val_accuracy: 0.5489\n",
            "Epoch 20/200\n",
            "8184/8184 [==============================] - 23s 3ms/step - loss: 0.6859 - accuracy: 0.5481 - val_loss: 0.6863 - val_accuracy: 0.5487\n",
            "Epoch 21/200\n",
            "8184/8184 [==============================] - 25s 3ms/step - loss: 0.6859 - accuracy: 0.5483 - val_loss: 0.6860 - val_accuracy: 0.5498\n",
            "Epoch 22/200\n",
            "8184/8184 [==============================] - 30s 4ms/step - loss: 0.6859 - accuracy: 0.5485 - val_loss: 0.6862 - val_accuracy: 0.5482\n",
            "Epoch 23/200\n",
            "8184/8184 [==============================] - 28s 3ms/step - loss: 0.6859 - accuracy: 0.5482 - val_loss: 0.6861 - val_accuracy: 0.5498\n",
            "Epoch 24/200\n",
            "8184/8184 [==============================] - 26s 3ms/step - loss: 0.6859 - accuracy: 0.5487 - val_loss: 0.6860 - val_accuracy: 0.5485\n",
            "Epoch 25/200\n",
            "8184/8184 [==============================] - 25s 3ms/step - loss: 0.6859 - accuracy: 0.5477 - val_loss: 0.6870 - val_accuracy: 0.5476\n",
            "4547/4547 [==============================] - 10s 2ms/step - loss: 0.6862 - accuracy: 0.5481\n",
            "Neural Network Model Accuracy: 0.5481294393539429\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjwAAAG2CAYAAAB7zFy5AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA6SklEQVR4nO3deXxTVf7/8XeStukilEKhC5YdEaUUZClV3KBaYWRkmRGQkYorCgh0HBBlERcq+AVRQXnAILiwCSPIDAwOVtEZRFCwoD8BkWUKQgvI0EKRLsn9/dE2bdqCtE0benk9H+SR3HPPTT65OWne3HtzYzEMwxAAAICJWb1dAAAAQHUj8AAAANMj8AAAANMj8AAAANMj8AAAANMj8AAAANMj8AAAANMj8AAAANMj8AAAANMj8AAAANPzauD54osv1KdPH0VGRspisWjNmjW/ucymTZt0ww03yG63q1WrVlq8eHG11wkAAGo3rwae7OxsxcTEaO7cuZfU/+DBg/rd736n22+/XampqRozZowefvhhffzxx9VcKQAAqM0sl8uPh1osFq1evVp9+/a9YJ/x48dr3bp1+v77711tgwYN0unTp7Vhw4YaqBIAANRGPt4uoCK2bNmi+Ph4t7aEhASNGTPmgsvk5OQoJyfHNe10OnXq1Ck1aNBAFoulukoFAAAeZBiGzpw5o8jISFmtFd9BVasCT3p6usLCwtzawsLClJWVpV9//VUBAQFllklOTtbUqVNrqkQAAFCNDh8+rKuvvrrCy9WqwFMZEyZMUFJSkms6MzNTTZo00eHDh1W3bl0vVgYAAC5VVlaWoqKiVKdOnUotX6sCT3h4uDIyMtzaMjIyVLdu3XK37kiS3W6X3W4v0163bl0CDwAAtUxlD0epVefhiYuLU0pKilvbxo0bFRcX56WKAABAbeDVwHP27FmlpqYqNTVVUsHXzlNTU5WWliapYHfU0KFDXf2HDx+uAwcOaNy4cdqzZ4/efPNNffDBBxo7dqw3ygcAALWEVwPPN998o44dO6pjx46SpKSkJHXs2FGTJ0+WJB07dswVfiSpefPmWrdunTZu3KiYmBjNnDlTf/3rX5WQkOCV+gEAQO1w2ZyHp6ZkZWUpODhYmZmZHMMDAEAtUdXP71p1DA8AAEBlEHgAAIDpEXgAAIDpEXgAAIDpEXgAAIDpEXgAAIDpEXgAAIDpEXgAAIDpEXgAAIDpEXgAAIDpEXgAAIDpEXgAAIDpEXgAAIDpEXgAAIDpEXgAAIDpEXgAAIDpEXgAAIDpEXgAAIDpEXgAAIDpEXgAAIDpEXgAAIDpEXgAAIDpEXgAAIDpEXgAAIDpEXgAAIDpEXgAAIDpEXgAAIDpEXgAAIDpEXgAAIDpEXgAAIDpEXgAAIDpEXgAAIDpEXgAAIDpEXgAAIDpEXgAAIDpEXgAAIDpEXgAAIDpEXgAAIDpEXgAAIDpEXgAAIDpEXgAAIDpEXgAAIDpEXgAAIDpEXgAAIDpEXgAAIDpEXgAAIDpEXgAAIDpEXgAAIDpEXgAAIDpEXgAAIDpEXgAAIDpEXgAAIDpEXgAAIDpEXgAAIDpEXgAAIDpEXgAAIDpEXgAAIDpEXgAAIDpEXgAAIDpEXgAAIDpEXgAAIDpEXgAAIDpEXgAAIDpeT3wzJ07V82aNZO/v79iY2O1bdu2i/afPXu22rRpo4CAAEVFRWns2LE6f/58DVULAABqI68GnhUrVigpKUlTpkzRjh07FBMTo4SEBB0/frzc/kuXLtXTTz+tKVOmaPfu3Vq4cKFWrFihZ555poYrBwAAtYlXA8+sWbP0yCOPaNiwYbruuus0b948BQYG6u233y63/5dffqmbbrpJ9913n5o1a6Y777xTgwcP/s2tQgAA4MrmtcCTm5ur7du3Kz4+vrgYq1Xx8fHasmVLucvceOON2r59uyvgHDhwQOvXr1fv3r0v+Dg5OTnKyspyuwAAgCuLj7ce+OTJk3I4HAoLC3NrDwsL0549e8pd5r777tPJkyfVvXt3GYah/Px8DR8+/KK7tJKTkzV16lSP1g4AAGoXrx+0XBGbNm3StGnT9Oabb2rHjh368MMPtW7dOr3wwgsXXGbChAnKzMx0XQ4fPlyDFQMAgMuB17bwhIaGymazKSMjw609IyND4eHh5S4zadIk3X///Xr44YclSdHR0crOztajjz6qZ599VlZr2fxmt9tlt9s9/wQAAECt4bUtPH5+furUqZNSUlJcbU6nUykpKYqLiyt3mXPnzpUJNTabTZJkGEb1FQsAAGo1r23hkaSkpCQlJiaqc+fO6tq1q2bPnq3s7GwNGzZMkjR06FA1btxYycnJkqQ+ffpo1qxZ6tixo2JjY/XTTz9p0qRJ6tOnjyv4AAAAlObVwDNw4ECdOHFCkydPVnp6ujp06KANGza4DmROS0tz26IzceJEWSwWTZw4UT///LMaNmyoPn366KWXXvLWUwAAALWAxbjC9gVlZWUpODhYmZmZqlu3rrfLAQAAl6Cqn9+16ltaAAAAlUHgAQAApkfgAQAApkfgAQAApkfgAQAApkfgAQAApkfgAQAApkfgAQAApkfgAQAApkfgAQAApkfgAQAApkfgAQAApkfgAQAApkfgAQAApkfgAQAApkfgAQAApkfgAQAApkfgAQAApkfgAQAApkfgAQAApkfgAQAApkfgAQAApkfgAQAApkfgAQAApkfgAQAApkfgAQAApkfgAQAApkfgAQAApkfgAQAApkfgAQAApkfgAQAApkfgAQAApkfgAQAApkfgAQAApkfgAQAApkfgAQAApkfgAQAApkfgAQAApkfgAQAApkfgAQAApkfgAQAApkfgAQAApkfgAQAApkfgAQAApkfgAQAApkfgAQAApkfgAQAApkfgAQAApkfgAQAApkfgAQAApkfgAQAApkfgAQAApkfgAQAApkfgAQAApkfgAQAApkfgAQAApkfgAQAApkfgAQAApkfgAQAApkfgAQAApkfgAQAApkfgAQAApkfgAQAApuf1wDN37lw1a9ZM/v7+io2N1bZt2y7a//Tp0xoxYoQiIiJkt9t1zTXXaP369TVULQAAqI18vPngK1asUFJSkubNm6fY2FjNnj1bCQkJ2rt3rxo1alSmf25uru644w41atRIq1atUuPGjfXf//5X9erVq/niAQBArWExDMPw1oPHxsaqS5cumjNnjiTJ6XQqKipKo0aN0tNPP12m/7x58/TKK69oz5498vX1rdRjZmVlKTg4WJmZmapbt26V6gcAADWjqp/fXtullZubq+3btys+Pr64GKtV8fHx2rJlS7nLrF27VnFxcRoxYoTCwsLUrl07TZs2TQ6H44KPk5OTo6ysLLcLAAC4sngt8Jw8eVIOh0NhYWFu7WFhYUpPTy93mQMHDmjVqlVyOBxav369Jk2apJkzZ+rFF1+84OMkJycrODjYdYmKivLo8wAAAJc/rx+0XBFOp1ONGjXS/Pnz1alTJw0cOFDPPvus5s2bd8FlJkyYoMzMTNfl8OHDNVgxAAC4HHjtoOXQ0FDZbDZlZGS4tWdkZCg8PLzcZSIiIuTr6yubzeZqa9u2rdLT05Wbmys/P78yy9jtdtntds8WDwAAahWvbeHx8/NTp06dlJKS4mpzOp1KSUlRXFxcucvcdNNN+umnn+R0Ol1tP/74oyIiIsoNOwAAAJKXd2klJSVpwYIFeuedd7R79249/vjjys7O1rBhwyRJQ4cO1YQJE1z9H3/8cZ06dUqjR4/Wjz/+qHXr1mnatGkaMWKEt54CAACoBbx6Hp6BAwfqxIkTmjx5stLT09WhQwdt2LDBdSBzWlqarNbiTBYVFaWPP/5YY8eOVfv27dW4cWONHj1a48eP99ZTAAAAtYBXz8PjDZyHBwCA2qfWnocHAACgplQ48DRr1kzPP/+80tLSqqMeAAAAj6tw4BkzZow+/PBDtWjRQnfccYeWL1+unJyc6qgNAADAIyoVeFJTU7Vt2za1bdtWo0aNUkREhEaOHKkdO3ZUR40AAABVUuWDlvPy8vTmm29q/PjxysvLU3R0tJ588kkNGzZMFovFU3V6DActAwBQ+1T187vSX0vPy8vT6tWrtWjRIm3cuFHdunXTQw89pCNHjuiZZ57RJ598oqVLl1b27gEAADymwoFnx44dWrRokZYtWyar1aqhQ4fq1Vdf1bXXXuvq069fP3Xp0sWjhQIAAFRWhQNPly5ddMcdd+itt95S37595evrW6ZP8+bNNWjQII8UCAAAUFUVDjwHDhxQ06ZNL9onKChIixYtqnRRAAAAnlThb2kdP35cW7duLdO+detWffPNNx4pCgAAwJMqvIVnxIgRGjdunGJjY93af/75Z02fPr3cMHRFOJMuff83SRbJYim8thbeVom2wna3fheYZ7VJVl/J5lt47VNi2qf4+oLziqZtktMpGQ7JmS85HYW3HZLhLDVdsr2cvobzAiughEv64p9R2M8ovN/Ca7fpkvMv1KdwumAlX2D9WstZ11bJUt4y1oJ1ZvMruPjYC6fthdN+xfMuw28hSipeL87C17vk6170epZe9263dYH20rdLvM5F67Xkbdf6ucTbRqnxaDgLx23R2HWUum2U3y6j8PW1FVxbbSWmLaWmS863ltO/aNpSok+p/harZC01XbqP1VpQb5nnUN5zKfU6lWlzFr63S47HwnHqY6+5sVk0zhx5kjOv+LlJZd+zbu/bku0Xeo8XDYty/ka6jZ0SbRea58wvqNGRJzlyS1znFs7LLdWe596n6Pk58grXub/k619wXXQpnDZ87DJs/jJ8/GXY7DJ8/QunC9stPjKKypVFFhmyGHmy5OfI4siRNf984e3zsjhypfzzhZccKe/Xguui6aJrw1E4BnyLx4HNT4bNV4bNLqfVV4bVV4bNTw6Lr5xWXzltfjKsvoXTBbfzrT6y2uyy+fjIx9cuXx+bfGw2+dosl+U3riujwoHnhx9+0A033FCmvWPHjvrhhx88UlRt9POhPWr88TPeLgM1KF8+yrf4Kt/io3yLn/LlozzXtK/y5VPwB02GVPjHTYXTxX+aC9uMwtsWyVIYJIqXLbhtNRyyyimrkS+bnLIYDtnkkNVwyiqHa76PHN5YHbiM5MnHNR7z5KM8FYzLouuC8ekrh8UmqxyyGU75KF9WwyEfOWRTvmyF48smh3yMfNftonZf5Xv7aV52SsT+cjkMi87LT/myyU/58lOerJbq+TnLolqq8vtReYZN5wtGgwr+6tiUbykYCUW3nZaCEeMsvF10nR3cWt1G/NVDz8YzKhx47Ha7MjIy1KJFC7f2Y8eOycfHqz++7lX/M+rqG8eNrg+pgkFffNvqul12XlG7tdQHnI+c8rHky1cFf4R8Cv/I+MghH4ujsD2/sL1wvqVyH3Z5hk2FH5sq+Ni0uG4XfqzKaRS0Gxd9S186Q5Kz8O3oVPF9F12chZfSbUbp9sL/Mlksxeuv4N7c163KXd/FocNaePFRvvws+fJTvnxVfF163fooXz5GfvEGplrAaRS8riWjV+l1WzQCS04XzVepvlLpYFa8Pt2npZIjp/QykuEafwWve9Ftq6tmZ+Gl+HZxe8nlDMMiq6WgwoI/y8W3C17jgqVLttssF+9TNDZsxY9ScPHwh1W+cbHnV/w8feSUX0F8KRifpcamb+GYDTDOFzd6YZw6jKK1pcK16v7eltzf5yXHWPHIK3+Muf8tVeG8sn9X82UrDH0+ypWP8gxbQRiUj3KL5hk+hX2K++bJR7mGj2v5fNnkq3zZlSt/S17BtfJ+e9qS51ofNouhIF34lwlyDF+dl69y5Kccw1c5KnEx/JQjX52XX+F0QbtTFvkWfjb4WfJdr71deQXtlqK/Y3nFf9NK9PMrvJQ3ln0LP2cuqPitXsae07kXXs5LKpxQ7rzzTk2YMEEfffSRgoODJUmnT5/WM888ozvuuMPjBdYWjZpfp6/verPgjWaxFGwBlySLRVZL4ebLwjZL4bRc05aCUGQt7icVbN11OA05DEPOktdOQw5DMgzDfb5TcjidkiNfhjNPlqLNsIZTslhlsdokq6342uIjWW2yWq2yWi2yFtZqsxZswixz2/W8ip5c8f9mijZ5Fj0/lbwu8ZxUon/xssX9iqZLz1OJ+yjvMZ2GIadRdF2wXkquv4J1VTzfWc46LOpvtajwORc8ptWignUjp2zOfPkYufKx5MvHyCucznNdrEaefJx5shkFl9LbdNwjlvufa6PwCRtGcT9nyf7WgtfLUMHrZ1h9ZFisBZvJrbaC3ShWn4JdKLaC/2kVtRmF16WfV9F00XO0FD3XonF7geuivpJc690oeg2cBddGqdfEMNxfp6L5DmfB/NKPW1yLe22lr4tu2yyST+G4MlTw+IYh5RW+l4pqMGSo8F+JtuL+hgzXHlWjxJgu/b51jXfDWRCwjMIQbThkMZySxZDVMApiS+F7sOD1sMmwWmWR1XVbloLXreQYLzm+i98XRWOleH0akgynUxZHrgxHbsGukFLXFkee5MiRxZEni7NEuzNPsvrIafEpHE8FY8mw+hZOF9622ArHn6+c1uK+shb2t/gWjsWCXXiGSu5+Kli3JZX5jDRKTxqFr2vx38/S46LkGLao7Ngo+rtjLVqPpf6u+MkivxLt5f4NK/W3y1Li8VyPVbiQ+994i5ySfrVI52XI4ii1y8qZK8PmL6fNT04ffxlWe8EuKFkL3guS/A3JXvj6lhynTqdR+L4rbrNZLLJaS7x3re7v45J/393aLRZZrZbCN0i+5MyXMz9Xefl5ys/LlSMvX/n5uXIUTefnyZmfp/z8XDkdea5ppyNfjvxcOR35Mgqn/a6qX/pV9roKn2n5559/1i233KJffvlFHTt2lCSlpqYqLCxMGzduVFRUVLUU6imcaRkAgNqnxs+03LhxY+3atUtLlizRzp07FRAQoGHDhmnw4MHlnpMHAADA2yp10E1QUJAeffRRT9cCAABQLSp9lPEPP/ygtLQ05ea6H5j0+9//vspFAQAAeFKlzrTcr18/fffdd7JYLK6D0YoO+HI4+EosAAC4vFT4K/qjR49W8+bNdfz4cQUGBur//b//py+++EKdO3fWpk2bqqFEAACAqqnwFp4tW7bo008/VWhoaOHXma3q3r27kpOT9eSTT+rbb7+tjjoBAAAqrcJbeBwOh+rUqSNJCg0N1dGjRyVJTZs21d69ez1bHQAAgAdUeAtPu3bttHPnTjVv3lyxsbGaMWOG/Pz8NH/+/DJnXwYAALgcVDjwTJw4UdnZ2ZKk559/XnfffbduvvlmNWjQQCtWrPB4gQAAAFVV4TMtl+fUqVMKCQmpFb+oypmWAQCofar6+V2hY3jy8vLk4+Oj77//3q29fv36tSLsAACAK1OFAo+vr6+aNGnCuXYAAECtUuFvaT377LN65plndOrUqeqoBwAAwOMqfNDynDlz9NNPPykyMlJNmzZVUFCQ2/wdO3Z4rDgAAABPqHDg6du3bzWUAQAAUH088i2t2oRvaQEAUPvU6Le0AAAAaqMK79KyWq0X/Qo63+ACAACXmwoHntWrV7tN5+Xl6dtvv9U777yjqVOneqwwAAAAT/HYMTxLly7VihUr9NFHH3ni7qoNx/AAAFD7XDbH8HTr1k0pKSmeujsAAACP8Ujg+fXXX/X666+rcePGnrg7AAAAj6rwMTylfyTUMAydOXNGgYGBev/99z1aHAAAgCdUOPC8+uqrboHHarWqYcOGio2NVUhIiEeLAwAA8IQKB54HHnigGsoAAACoPhU+hmfRokVauXJlmfaVK1fqnXfe8UhRAAAAnlThwJOcnKzQ0NAy7Y0aNdK0adM8UhQAAIAnVTjwpKWlqXnz5mXamzZtqrS0NI8UBQAA4EkVDjyNGjXSrl27yrTv3LlTDRo08EhRAAAAnlThwDN48GA9+eST+uyzz+RwOORwOPTpp59q9OjRGjRoUHXUCAAAUCUV/pbWCy+8oEOHDqlnz57y8SlY3Ol0aujQoRzDAwAALkuV/i2tffv2KTU1VQEBAYqOjlbTpk09XVu14Le0AACofar6+V3hLTxFWrdurdatW1d2cQAAgBpT4WN4BgwYoOnTp5dpnzFjhv74xz96pCgAAABPqnDg+eKLL9S7d+8y7b169dIXX3zhkaIAAAA8qcKB5+zZs/Lz8yvT7uvrq6ysLI8UBQAA4EkVDjzR0dFasWJFmfbly5fruuuu80hRAAAAnlThg5YnTZqk/v37a//+/erRo4ckKSUlRUuXLtWqVas8XiAAAEBVVTjw9OnTR2vWrNG0adO0atUqBQQEKCYmRp9++qnq169fHTUCAABUSaXPw1MkKytLy5Yt08KFC7V9+3Y5HA5P1VYtOA8PAAC1T1U/vyt8DE+RL774QomJiYqMjNTMmTPVo0cPffXVV5W9OwAAgGpToV1a6enpWrx4sRYuXKisrCzde++9ysnJ0Zo1azhgGQAAXLYueQtPnz591KZNG+3atUuzZ8/W0aNH9cYbb1RnbQAAAB5xyVt4/vnPf+rJJ5/U448/zk9KAACAWuWSt/D85z//0ZkzZ9SpUyfFxsZqzpw5OnnyZHXWBgAA4BGXHHi6deumBQsW6NixY3rssce0fPlyRUZGyul0auPGjTpz5kx11gkAAFBpVfpa+t69e7Vw4UK99957On36tO644w6tXbvWk/V5HF9LBwCg9vHa19IlqU2bNpoxY4aOHDmiZcuWVeWuAAAAqk2VAk8Rm82mvn37Vnrrzty5c9WsWTP5+/srNjZW27Ztu6Tlli9fLovFor59+1bqcQEAwJXBI4GnKlasWKGkpCRNmTJFO3bsUExMjBISEnT8+PGLLnfo0CE99dRTuvnmm2uoUgAAUFt5PfDMmjVLjzzyiIYNG6brrrtO8+bNU2BgoN5+++0LLuNwODRkyBBNnTpVLVq0qMFqAQBAbeTVwJObm6vt27crPj7e1Wa1WhUfH68tW7ZccLnnn39ejRo10kMPPfSbj5GTk6OsrCy3CwAAuLJ4NfCcPHlSDodDYWFhbu1hYWFKT08vd5n//Oc/WrhwoRYsWHBJj5GcnKzg4GDXJSoqqsp1AwCA2sXru7Qq4syZM7r//vu1YMEChYaGXtIyEyZMUGZmputy+PDhaq4SAABcbir046GeFhoaKpvNpoyMDLf2jIwMhYeHl+m/f/9+HTp0SH369HG1OZ1OSZKPj4/27t2rli1bui1jt9tlt9uroXoAAFBbeHULj5+fnzp16qSUlBRXm9PpVEpKiuLi4sr0v/baa/Xdd98pNTXVdfn973+v22+/XampqeyuAgAA5fLqFh5JSkpKUmJiojp37qyuXbtq9uzZys7O1rBhwyRJQ4cOVePGjZWcnCx/f3+1a9fObfl69epJUpl2AACAIl4PPAMHDtSJEyc0efJkpaenq0OHDtqwYYPrQOa0tDRZrbXqUCMAAHCZqdJvadVG/JYWAAC1j1d/SwsAAKA2IPAAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTuywCz9y5c9WsWTP5+/srNjZW27Ztu2DfBQsW6Oabb1ZISIhCQkIUHx9/0f4AAABeDzwrVqxQUlKSpkyZoh07digmJkYJCQk6fvx4uf03bdqkwYMH67PPPtOWLVsUFRWlO++8Uz///HMNVw4AAGoLi2EYhjcLiI2NVZcuXTRnzhxJktPpVFRUlEaNGqWnn376N5d3OBwKCQnRnDlzNHTo0N/sn5WVpeDgYGVmZqpu3bpVrh8AAFS/qn5+e3ULT25urrZv3674+HhXm9VqVXx8vLZs2XJJ93Hu3Dnl5eWpfv365c7PyclRVlaW2wUAAFxZvBp4Tp48KYfDobCwMLf2sLAwpaenX9J9jB8/XpGRkW6hqaTk5GQFBwe7LlFRUVWuGwAA1C5eP4anKl5++WUtX75cq1evlr+/f7l9JkyYoMzMTNfl8OHDNVwlAADwNh9vPnhoaKhsNpsyMjLc2jMyMhQeHn7RZf/v//5PL7/8sj755BO1b9/+gv3sdrvsdrtH6gUAALWTV7fw+Pn5qVOnTkpJSXG1OZ1OpaSkKC4u7oLLzZgxQy+88II2bNigzp0710SpAACgFvPqFh5JSkpKUmJiojp37qyuXbtq9uzZys7O1rBhwyRJQ4cOVePGjZWcnCxJmj59uiZPnqylS5eqWbNmrmN9rrrqKl111VVeex4AAODy5fXAM3DgQJ04cUKTJ09Wenq6OnTooA0bNrgOZE5LS5PVWrwh6q233lJubq7+8Ic/uN3PlClT9Nxzz9Vk6QAAoJbw+nl4ahrn4QEAoPap1efhAQAAqAkEHgAAYHoEHgAAYHoEHgAAYHoEHgAAYHoEHgAAYHoEHgAAYHoEHgAAYHoEHgAAYHoEHgAAYHoEHgAAYHoEHgAAYHoEHgAAYHo+3i4AAGB+DodDeXl53i4DlzlfX1/ZbLZquW8CDwCgWp09e1ZHjhyRYRjeLgWXOYvFoquvvlpXXXWVx++bwAMAqDYOh0NHjhxRYGCgGjZsKIvF4u2ScJkyDEMnTpzQkSNH1Lp1a49v6SHwAACqTV5engzDUMOGDRUQEODtcnCZa9iwoQ4dOqS8vDyPBx4OWgYAVDu27OBSVOc4IfAAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAFALcOLGqiHwAABqjGEYOpeb75VLRU98uGHDBnXv3l316tVTgwYNdPfdd2v//v2u+UeOHNHgwYNVv359BQUFqXPnztq6datr/t///nd16dJF/v7+Cg0NVb9+/VzzLBaL1qxZ4/Z49erV0+LFiyVJhw4dksVi0YoVK3TrrbfK399fS5Ys0S+//KLBgwercePGCgwMVHR0tJYtW+Z2P06nUzNmzFCrVq1kt9vVpEkTvfTSS5KkHj16aOTIkW79T5w4IT8/P6WkpFRo/dQ2nIcHAFBjfs1z6LrJH3vlsX94PkGBfpf+sZedna2kpCS1b99eZ8+e1eTJk9WvXz+lpqbq3LlzuvXWW9W4cWOtXbtW4eHh2rFjh5xOpyRp3bp16tevn5599lm9++67ys3N1fr16ytc89NPP62ZM2eqY8eO8vf31/nz59WpUyeNHz9edevW1bp163T//ferZcuW6tq1qyRpwoQJWrBggV599VV1795dx44d0549eyRJDz/8sEaOHKmZM2fKbrdLkt5//301btxYPXr0qHB9tQmBBwCAcgwYMMBt+u2331bDhg31ww8/6Msvv9SJEyf09ddfq379+pKkVq1aufq+9NJLGjRokKZOnepqi4mJqXANY8aMUf/+/d3annrqKdftUaNG6eOPP9YHH3ygrl276syZM3rttdc0Z84cJSYmSpJatmyp7t27S5L69++vkSNH6qOPPtK9994rSVq8eLEeeOAB058ricADAKgxAb42/fB8gtceuyL27dunyZMna+vWrTp58qRr601aWppSU1PVsWNHV9gpLTU1VY888kiVa+7cubPbtMPh0LRp0/TBBx/o559/Vm5urnJychQYGChJ2r17t3JyctSzZ89y78/f31/333+/3n77bd17773asWOHvv/+e61du7bKtV7uCDwAgBpjsVgqtFvJm/r06aOmTZtqwYIFioyMlNPpVLt27ZSbm/ubP5PxW/MtFkuZY4rKOyg5KCjIbfqVV17Ra6+9ptmzZys6OlpBQUEaM2aMcnNzL+lxpYLdWh06dNCRI0e0aNEi9ejRQ02bNv3N5Wo7DloGAKCUX375RXv37tXEiRPVs2dPtW3bVv/73/9c89u3b6/U1FSdOnWq3OXbt29/0YOAGzZsqGPHjrmm9+3bp3Pnzv1mXZs3b9Y999yjP/3pT4qJiVGLFi30448/uua3bt1aAQEBF33s6Ohode7cWQsWLNDSpUv14IMP/ubjmgGBBwCAUkJCQtSgQQPNnz9fP/30kz799FMlJSW55g8ePFjh4eHq27evNm/erAMHDuhvf/ubtmzZIkmaMmWKli1bpilTpmj37t367rvvNH36dNfyPXr00Jw5c/Ttt9/qm2++0fDhw+Xr6/ubdbVu3VobN27Ul19+qd27d+uxxx5TRkaGa76/v7/Gjx+vcePG6d1339X+/fv11VdfaeHChW738/DDD+vll1+WYRhu3x4zMwIPAAClWK1WLV++XNu3b1e7du00duxYvfLKK675fn5++te//qVGjRqpd+/eio6O1ssvv+z6he/bbrtNK1eu1Nq1a9WhQwf16NFD27Ztcy0/c+ZMRUVF6eabb9Z9992np556ynUczsVMnDhRN9xwgxISEnTbbbe5QldJkyZN0p///GdNnjxZbdu21cCBA3X8+HG3PoMHD5aPj48GDx4sf3//Kqyp2sNiVPTEBLVcVlaWgoODlZmZqbp163q7HAAwtfPnz+vgwYNq3rz5FfPBWhscOnRILVu21Ndff60bbrjB2+W4XGy8VPXzu3YcOQYAAKosLy9Pv/zyiyZOnKhu3bpdVmGnurFLCwCAK8TmzZsVERGhr7/+WvPmzfN2OTWKLTwAAFwhbrvttgr/xIZZsIUHAACYHoEHAACYHoEHAACYHoEHAACYHoEHAACYHoEHAACYHoEHAIBq0KxZM82ePdvbZaAQgQcAAJgegQcAALhxOBxyOp3eLsOjCDwAgJpjGFJutncuFTjD8Pz58xUZGVnmQ/+ee+7Rgw8+qP379+uee+5RWFiYrrrqKnXp0kWffPJJpVfLrFmzFB0draCgIEVFRemJJ57Q2bNn3fps3rxZt912mwIDAxUSEqKEhAT973//kyQ5nU7NmDFDrVq1kt1uV5MmTfTSSy9JkjZt2iSLxaLTp0+77is1NVUWi0WHDh2SJC1evFj16tXT2rVrdd1118lutystLU1ff/217rjjDoWGhio4OFi33nqrduzY4VbX6dOn9dhjjyksLEz+/v5q166d/vGPfyg7O1t169bVqlWr3PqvWbNGQUFBOnPmTKXXV2Xw0xIAgJqTd06aFumdx37mqOQXdEld//jHP2rUqFH67LPP1LNnT0nSqVOntGHDBq1fv15nz55V79699dJLL8lut+vdd99Vnz59tHfvXjVp0qTCpVmtVr3++utq3ry5Dhw4oCeeeELjxo3Tm2++KakgoPTs2VMPPvigXnvtNfn4+Oizzz6Tw+GQJE2YMEELFizQq6++qu7du+vYsWPas2dPhWo4d+6cpk+frr/+9a9q0KCBGjVqpAMHDigxMVFvvPGGDMPQzJkz1bt3b+3bt0916tSR0+lUr169dObMGb3//vtq2bKlfvjhB9lsNgUFBWnQoEFatGiR/vCHP7gep2i6Tp06FV5PVUHgAQCglJCQEPXq1UtLly51BZ5Vq1YpNDRUt99+u6xWq2JiYlz9X3jhBa1evVpr167VyJEjK/x4Y8aMcd1u1qyZXnzxRQ0fPtwVeGbMmKHOnTu7piXp+uuvlySdOXNGr732mubMmaPExERJUsuWLdW9e/cK1ZCXl6c333zT7Xn16NHDrc/8+fNVr149ff7557r77rv1ySefaNu2bdq9e7euueYaSVKLFi1c/R9++GHdeOONOnbsmCIiInT8+HGtX7++SlvDKovAAwCoOb6BBVtavPXYFTBkyBA98sgjevPNN2W327VkyRINGjRIVqtVZ8+e1XPPPad169bp2LFjys/P16+//qq0tLRKlfbJJ58oOTlZe/bsUVZWlvLz83X+/HmdO3dOgYGBSk1N1R//+Mdyl929e7dycnJcwayy/Pz81L59e7e2jIwMTZw4UZs2bdLx48flcDh07tw51/NMTU3V1Vdf7Qo7pXXt2lXXX3+93nnnHT399NN6//331bRpU91yyy1VqrUyOIYHAFBzLJaC3UreuFgsFSq1T58+MgxD69at0+HDh/Xvf/9bQ4YMkSQ99dRTWr16taZNm6Z///vfSk1NVXR0tHJzcyu8Sg4dOqS7775b7du319/+9jdt375dc+fOlSTX/QUEBFxw+YvNkwp2l0ly+5X0vLy8cu/HUmodJSYmKjU1Va+99pq+/PJLpaamqkGDBpdUV5GHH35YixcvllSwO2vYsGFlHqcmEHgAACiHv7+/+vfvryVLlmjZsmVq06aNbrjhBkkFBxA/8MAD6tevn6KjoxUeHu46ALiitm/fLqfTqZkzZ6pbt2665pprdPSo+1aw9u3bKyUlpdzlW7durYCAgAvOb9iwoSTp2LFjrrbU1NRLqm3z5s168skn1bt3b11//fWy2+06efKkW11HjhzRjz/+eMH7+NOf/qT//ve/ev311/XDDz+4drvVNAIPAAAXMGTIEK1bt05vv/22a+uOVBAyPvzwQ6Wmpmrnzp267777Kv017latWikvL09vvPGGDhw4oPfee0/z5s1z6zNhwgR9/fXXeuKJJ7Rr1y7t2bNHb731lk6ePCl/f3+NHz9e48aN07vvvqv9+/frq6++0sKFC133HxUVpeeee0779u3TunXrNHPmzEuqrXXr1nrvvfe0e/dubd26VUOGDHHbqnPrrbfqlltu0YABA7Rx40YdPHhQ//znP7VhwwZXn5CQEPXv319/+ctfdOedd+rqq6+u1HqqKgIPAAAX0KNHD9WvX1979+7Vfffd52qfNWuWQkJCdOONN6pPnz5KSEhwbf2pqJiYGM2aNUvTp09Xu3bttGTJEiUnJ7v1ueaaa/Svf/1LO3fuVNeuXRUXF6ePPvpIPj4Fh+JOmjRJf/7znzV58mS1bdtWAwcO1PHjxyVJvr6+WrZsmfbs2aP27dtr+vTpevHFFy+ptoULF+p///ufbrjhBt1///168skn1ahRI7c+f/vb39SlSxcNHjxY1113ncaNG+f69liRhx56SLm5uXrwwQcrtY48wWIYFTgxgQlkZWUpODhYmZmZqlu3rrfLAQBTO3/+vA4ePKjmzZvL39/f2+XAS9577z2NHTtWR48elZ+f3wX7XWy8VPXzm29pAQCAanHu3DkdO3ZML7/8sh577LGLhp3qxi4tAACq0ZIlS3TVVVeVeyk6l45ZzZgxQ9dee63Cw8M1YcIEr9bCLi0AQLVhl1bBiQEzMjLKnefr66umTZvWcEWXL3ZpAQBQS9WpU6fGf0YBZbFLCwBQ7a6wnQmopOocJwQeAEC1sdlsklSpMxDjylM0TorGjSexSwsAUG18fHwUGBioEydOyNfX1/UzB0BpTqdTJ06cUGBgoOv8Qp5E4AEAVBuLxaKIiAgdPHhQ//3vf71dDi5zVqtVTZo0qZbf2iLwAACqlZ+fn1q3bs1uLfwmPz+/atsKSOABAFQ7q9V6xX4tHZeHy2Jn6ty5c9WsWTP5+/srNjZW27Ztu2j/lStX6tprr5W/v7+io6O1fv36GqoUAADURl4PPCtWrFBSUpKmTJmiHTt2KCYmRgkJCa4fPSvtyy+/1ODBg/XQQw/p22+/Vd++fdW3b199//33NVw5AACoLbx+puXY2Fh16dJFc+bMkVRwlHZUVJRGjRqlp59+ukz/gQMHKjs7W//4xz9cbd26dVOHDh00b96833w8zrQMAEDtU6vPtJybm6vt27e7/b6G1WpVfHy8tmzZUu4yW7ZsUVJSkltbQkKC1qxZU27/nJwc5eTkuKYzMzMlFaw4AABQOxR9bld2O41XA8/JkyflcDgUFhbm1h4WFqY9e/aUu0x6enq5/dPT08vtn5ycrKlTp5Zpj4qKqmTVAADAW86cOaPg4OAKL2f6b2lNmDDBbYuQ0+nUqVOn1KBBA49/zz8rK0tRUVE6fPgwu8tqEOvdO1jv3sF69w7Wu3eUXO916tTRmTNnFBkZWan78mrgCQ0Nlc1mK/MrshkZGQoPDy93mfDw8Ar1t9vtstvtbm316tWrfNGXoG7durwhvID17h2sd+9gvXsH6907itZ7ZbbsFPHqt7T8/PzUqVMnpaSkuNqcTqdSUlIUFxdX7jJxcXFu/SVp48aNF+wPAADg9V1aSUlJSkxMVOfOndW1a1fNnj1b2dnZGjZsmCRp6NChaty4sZKTkyVJo0eP1q233qqZM2fqd7/7nZYvX65vvvlG8+fP9+bTAAAAlzGvB56BAwfqxIkTmjx5stLT09WhQwdt2LDBdWByWlqa22mmb7zxRi1dulQTJ07UM888o9atW2vNmjVq166dt56Ci91u15QpU8rsQkP1Yr17B+vdO1jv3sF69w5Prnevn4cHAACgunn9TMsAAADVjcADAABMj8ADAABMj8ADAABMj8DjIXPnzlWzZs3k7++v2NhYbdu2zdslmdpzzz0ni8Xidrn22mu9XZbpfPHFF+rTp48iIyNlsVjK/GadYRiaPHmyIiIiFBAQoPj4eO3bt887xZrIb633Bx54oMz4v+uuu7xTrIkkJyerS5cuqlOnjho1aqS+fftq7969bn3Onz+vESNGqEGDBrrqqqs0YMCAMifDRcVcynq/7bbbyoz54cOHV+hxCDwesGLFCiUlJWnKlCnasWOHYmJilJCQoOPHj3u7NFO7/vrrdezYMdflP//5j7dLMp3s7GzFxMRo7ty55c6fMWOGXn/9dc2bN09bt25VUFCQEhISdP78+Rqu1Fx+a71L0l133eU2/pctW1aDFZrT559/rhEjRuirr77Sxo0blZeXpzvvvFPZ2dmuPmPHjtXf//53rVy5Up9//rmOHj2q/v37e7Hq2u9S1rskPfLII25jfsaMGRV7IANV1rVrV2PEiBGuaYfDYURGRhrJyclerMrcpkyZYsTExHi7jCuKJGP16tWuaafTaYSHhxuvvPKKq+306dOG3W43li1b5oUKzan0ejcMw0hMTDTuuecer9RzJTl+/Lghyfj8888NwygY376+vsbKlStdfXbv3m1IMrZs2eKtMk2n9Ho3DMO49dZbjdGjR1fpftnCU0W5ubnavn274uPjXW1Wq1Xx8fHasmWLFyszv3379ikyMlItWrTQkCFDlJaW5u2SrigHDx5Uenq629gPDg5WbGwsY78GbNq0SY0aNVKbNm30+OOP65dffvF2SaaTmZkpSapfv74kafv27crLy3Mb89dee62aNGnCmPeg0uu9yJIlSxQaGqp27dppwoQJOnfuXIXu1+tnWq7tTp48KYfD4TozdJGwsDDt2bPHS1WZX2xsrBYvXqw2bdro2LFjmjp1qm6++WZ9//33qlOnjrfLuyKkp6dLUrljv2geqsddd92l/v37q3nz5tq/f7+eeeYZ9erVS1u2bJHNZvN2eabgdDo1ZswY3XTTTa4z+aenp8vPz6/MD1Az5j2nvPUuSffdd5+aNm2qyMhI7dq1S+PHj9fevXv14YcfXvJ9E3hQK/Xq1ct1u3379oqNjVXTpk31wQcf6KGHHvJiZUD1GzRokOt2dHS02rdvr5YtW2rTpk3q2bOnFyszjxEjRuj777/n2MAadqH1/uijj7puR0dHKyIiQj179tT+/fvVsmXLS7pvdmlVUWhoqGw2W5mj9DMyMhQeHu6lqq489erV0zXXXKOffvrJ26VcMYrGN2Pf+1q0aKHQ0FDGv4eMHDlS//jHP/TZZ5/p6quvdrWHh4crNzdXp0+fduvPmPeMC6338sTGxkpShcY8gaeK/Pz81KlTJ6WkpLjanE6nUlJSFBcX58XKrixnz57V/v37FRER4e1SrhjNmzdXeHi429jPysrS1q1bGfs17MiRI/rll18Y/1VkGIZGjhyp1atX69NPP1Xz5s3d5nfq1Em+vr5uY37v3r1KS0tjzFfBb6338qSmpkpShcY8u7Q8ICkpSYmJiercubO6du2q2bNnKzs7W8OGDfN2aab11FNPqU+fPmratKmOHj2qKVOmyGazafDgwd4uzVTOnj3r9j+ogwcPKjU1VfXr11eTJk00ZswYvfjii2rdurWaN2+uSZMmKTIyUn379vVe0SZwsfVev359TZ06VQMGDFB4eLj279+vcePGqVWrVkpISPBi1bXfiBEjtHTpUn300UeqU6eO67ic4OBgBQQEKDg4WA899JCSkpJUv3591a1bV6NGjVJcXJy6devm5eprr99a7/v379fSpUvVu3dvNWjQQLt27dLYsWN1yy23qH379pf+QFX6jhdc3njjDaNJkyaGn5+f0bVrV+Orr77ydkmmNnDgQCMiIsLw8/MzGjdubAwcOND46aefvF2W6Xz22WeGpDKXxMREwzAKvpo+adIkIywszLDb7UbPnj2NvXv3erdoE7jYej937pxx5513Gg0bNjR8fX2Npk2bGo888oiRnp7u7bJrvfLWuSRj0aJFrj6//vqr8cQTTxghISFGYGCg0a9fP+PYsWPeK9oEfmu9p6WlGbfccotRv359w263G61atTL+8pe/GJmZmRV6HEvhgwEAAJgWx/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAAADTI/AAuOJZLBatWbPG22UAqEYEHgBe9cADD8hisZS53HXXXd4uDYCJ8FtaALzurrvu0qJFi9za7Ha7l6oBYEZs4QHgdXa7XeHh4W6XkJAQSQW7m9566y316tVLAQEBatGihVatWuW2/HfffacePXooICBADRo00KOPPqqzZ8+69Xn77bd1/fXXy263KyIiQiNHjnSbf/LkSfXr10+BgYFq3bq11q5dW71PGkCNIvAAuOxNmjRJAwYM0M6dOzVkyBANGjRIu3fvliRlZ2crISFBISEh+vrrr7Vy5Up98sknboHmrbfe0ogRI/Too4/qu+++09q1a9WqVSu3x5g6daruvfde7dq1S71799aQIUN06tSpGn2eAKqRx3/2FAAqIDEx0bDZbEZQUJDb5aWXXjIMo+CXlIcPH+62TGxsrPH4448bhmEY8+fPN0JCQoyzZ8+65q9bt86wWq2uXxCPjIw0nn322QvWIMmYOHGia/rs2bOGJOOf//ynx54nAO/iGB4AXnf77bfrrbfecmurX7++63ZcXJzbvLi4OKWmpkqSdu/erZiYGAUFBbnm33TTTXI6ndq7d68sFouOHj2qnj17XrSG9u3bu24HBQWpbt26On78eGWfEoDLDIEHgNcFBQWV2cXkKQEBAZfUz9fX123aYrHI6XRWR0kAvIBjeABc9r766qsy023btpUktW3bVjt37lR2drZr/ubNm2W1WtWmTRvVqVNHzZo1U0pKSo3WDODywhYeAF6Xk5Oj9PR0tzYfHx+FhoZKklauXKnOnTure/fuWrJkibZt26aFCxdKkoYMGaIpU6YoMTFRzz33nE6cOKFRo0bp/vvvV1hYmCTpueee0/Dhw9WoUSP16tVLZ86c0ebNmzVq1KiafaIAvIbAA8DrNmzYoIiICLe2Nm3aaM+ePZIKvkG1fPlyPfHEE4qIiNCyZct03XXXSZICAwP18ccfa/To0erSpYsCAwM1YMAAzZo1y3VfiYmJOn/+vF599VU99dRTCg0N1R/+8Ieae4IAvM5iGIbh7SIA4EIsFotWr16tvn37ersUALUYx/AAAADTI/AAAADT4xgeAJc19roD8AS28AAAANMj8AAAANMj8AAAANMj8AAAANMj8AAAANMj8AAAANMj8AAAANMj8AAAANMj8AAAANP7/+0F2A8PTjFpAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2SfvP7kVq-v"
      },
      "outputs": [],
      "source": [
        "# Backtesting Strategy with Balance Check, Stop Loss, and Target Profit\n",
        "def backtest_strategy(model, data, initial_balance=100000, stop_loss_pct=0.02, target_profit_pct=0.05):\n",
        "    balance = initial_balance\n",
        "    position = None\n",
        "    buy_price = 0\n",
        "    trade_log = []\n",
        "\n",
        "    for index, row in data.iterrows():\n",
        "        if position is None:\n",
        "            features = row[['SMA_50', 'SMA_200', 'RSI', 'Bollinger_Upper', 'Bollinger_Lower', 'MACD', 'MACD_Signal', 'Stochastic_%K', 'Stochastic_%D', 'Momentum', 'VWAP']].values.reshape(1, -1)\n",
        "            prediction = model.predict(features)[0]\n",
        "\n",
        "            if prediction == 1:\n",
        "                buy_price = row['close']\n",
        "                position = 'long'\n",
        "                trade_log.append((index, 'buy', buy_price))\n",
        "                print(f\"Buying at {buy_price}\")\n",
        "\n",
        "        elif position == 'long':\n",
        "            current_price = row['close']\n",
        "            if current_price <= buy_price * (1 - stop_loss_pct):\n",
        "                balance -= (buy_price - current_price)\n",
        "                position = None\n",
        "                trade_log.append((index, 'stop_loss', current_price))\n",
        "                print(f\"Stop loss at {current_price}\")\n",
        "            elif current_price >= buy_price * (1 + target_profit_pct):\n",
        "                balance += (current_price - buy_price)\n",
        "                position = None\n",
        "                trade_log.append((index, 'target_profit', current_price))\n",
        "                print(f\"Target profit at {current_price}\")\n",
        "\n",
        "    return balance, trade_log\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Fetch historical data for backtest\n",
        "\n",
        "# Authentication and data fetching\n",
        "import pandas as pd\n",
        "from kiteconnect import KiteConnect\n",
        "import datetime as dt\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "api_key = 'klz728yv89qrljzs'\n",
        "api_secret = '4vhxunujbp17i8da0y1tiy7ayde4h5o8'\n",
        "kite = KiteConnect(api_key=api_key)\n",
        "print(\"Login URL:\", kite.login_url())\n",
        "request_token = input(\"Enter request token: \")\n",
        "data = kite.generate_session(request_token, api_secret=api_secret)\n",
        "access_token = data[\"access_token\"]\n",
        "kite.set_access_token(access_token)\n",
        "\n",
        "# Fetch historical data\n",
        "def fetch_historical_data(kite, instrument_token, start_date, end_date, interval, csv_filename):\n",
        "    delta = dt.timedelta(days=60)\n",
        "    current_date = start_date\n",
        "    all_data = []\n",
        "\n",
        "    while current_date < end_date:\n",
        "        to_date = min(current_date + delta, end_date)\n",
        "        data = kite.historical_data(instrument_token, current_date, to_date, interval)\n",
        "        all_data.extend(data)\n",
        "        current_date = to_date + dt.timedelta(days=1)\n",
        "        time.sleep(1)  # Avoid hitting API rate limits\n",
        "\n",
        "    df = pd.DataFrame(all_data)\n",
        "    df.to_csv(csv_filename, mode='w', index=False, header=True)\n",
        "    return df\n",
        "\n",
        "instrument_token = '738561' #INFY\n",
        "start_date = dt.datetime(2017, 1, 1)\n",
        "end_date = dt.datetime(2023, 12, 31)\n",
        "interval = 'minute'\n",
        "csv_filename = 'historical_data.csv'\n",
        "\n",
        "df_backtest = fetch_historical_data(kite, instrument_token, start_date, end_date, interval, 'backtest_data.csv')\n",
        "df_backtest = add_features(df_backtest)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "RC6Z-t93lRsO",
        "outputId": "c3405f0a-db4e-4513-8b7b-7d47aa4838a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'fetch_historical_data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-740025c61a6a>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mcsv_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'historical_data.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdf_backtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_historical_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstrument_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'backtest_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mdf_backtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_backtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'fetch_historical_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Backtesting with Random Forest Model\n",
        "final_balance, trade_log = backtest_strategy(rf, df_backtest)\n",
        "print(f\"Final Balance (Random Forest): {final_balance}\")\n",
        "print(trade_log)\n",
        "\n"
      ],
      "metadata": {
        "id": "dqYEJHs6ipCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Backtesting with Neural Network Model\n",
        "final_balance, trade_log = backtest_strategy(nn_model, df_backtest)\n",
        "print(f\"Final Balance (Neural Network): {final_balance}\")\n",
        "print(trade_log)"
      ],
      "metadata": {
        "id": "_uq2gw3bWhg6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}